from flask import Flask, render_template, jsonify, request, send_from_directory
import json
import tweepy
import os
import secrets
from datetime import datetime, timedelta, timezone
import sqlite3
import logging
import threading
import time

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', secrets.token_hex(32))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Twitter APIé…ç½®
TWITTER_BEARER_TOKEN = os.environ.get('TWITTER_BEARER_TOKEN')

class TwitterAPI:
    def __init__(self):
        if TWITTER_BEARER_TOKEN:
            try:
                self.client = tweepy.Client(bearer_token=TWITTER_BEARER_TOKEN)
                logger.info("âœ… Twitter APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                self.test_connection()
            except Exception as e:
                logger.error(f"âŒ Twitter APIåˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
        else:
            self.client = None
            logger.warning("âš ï¸ Twitter Bearer Tokenæœªé…ç½®ï¼Œå°†æ— æ³•è·å–çœŸå®æ•°æ®")
    
    def test_connection(self):
        """æµ‹è¯•APIè¿æ¥"""
        try:
            if self.client:
                # æµ‹è¯•è·å–ä¸€ä¸ªå…¬å¼€ç”¨æˆ·ä¿¡æ¯
                user = self.client.get_user(username='elonmusk')
                if user.data:
                    logger.info("ğŸ”— Twitter APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                    return True
        except Exception as e:
            logger.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    def get_user_tweets(self, username, max_results=10):
        """
        è·å–ç”¨æˆ·è‡ª2025å¹´1æœˆ1æ—¥ä»¥æ¥çš„æ¨æ–‡ã€‚
        å¦‚æœè·å–å¤±è´¥æˆ–æ²¡æœ‰æ–°æ¨æ–‡ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        if not self.client:
            logger.warning(f"Twitterå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è·å– {username} çš„æ¨æ–‡")
            return []
        
        try:
            # ç§»é™¤@ç¬¦å·
            username = username.replace('@', '')
            
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_response = self.client.get_user(username=username)
            if not user_response.data:
                logger.warning(f"Twitterç”¨æˆ· {username} ä¸å­˜åœ¨")
                return []
            
            user_id = user_response.data.id
            
            # è®¾ç½®èµ·å§‹æ—¶é—´ä¸º2025å¹´1æœˆ1æ—¥
            start_date = datetime(2025, 1, 1, tzinfo=timezone.utc)
            
            # è·å–æ¨æ–‡
            tweets_response = self.client.get_users_tweets(
                id=user_id,
                max_results=min(max_results, 100),
                tweet_fields=['created_at', 'public_metrics', 'context_annotations'],
                exclude=['retweets', 'replies'],
                start_time=start_date
            )
            
            if not tweets_response.data:
                logger.info(f"âœ… æœªæ‰¾åˆ° {username} è‡ª {start_date.date()} ä»¥æ¥çš„æ–°æ¨æ–‡")
                return []
            
            result = []
            for tweet in tweets_response.data:
                result.append({
                    'id': str(tweet.id),
                    'content': tweet.text,
                    'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                    'likes': tweet.public_metrics.get('like_count', 0),
                    'retweets': tweet.public_metrics.get('retweet_count', 0),
                    'replies': tweet.public_metrics.get('reply_count', 0),
                    'author': username,
                    'type': 'text'
                })
            
            logger.info(f"âœ… æˆåŠŸè·å– {username} çš„ {len(result)} æ¡æ–°æ¨æ–‡")
            return result
            
        except Exception as e:
            logger.error(f"è·å– {username} æ¨æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return [] # å‘ç”Ÿä»»ä½•é”™è¯¯æ—¶ï¼Œè¿”å›ç©ºåˆ—è¡¨

class ResearcherManager:
    def __init__(self):
        self.init_database()
        self.load_sample_data()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ - æ”¯æŒå¤§è§„æ¨¡æ•°æ®å­˜å‚¨"""
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # å¼€å¯å¤–é”®çº¦æŸå’ŒåŸºæœ¬ä¼˜åŒ–è®¾ç½®
        cursor.execute("PRAGMA foreign_keys = ON;")
        cursor.execute("PRAGMA synchronous = NORMAL;")  # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§

        # ç ”ç©¶è€…è¡¨ - ä¼˜åŒ–å­—æ®µç±»å‹å’Œç´¢å¼•
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS researchers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rank INTEGER,
                name TEXT NOT NULL,
                country TEXT,
                company TEXT,
                research_focus TEXT,
                x_account TEXT,
                followers_count TEXT DEFAULT '0',
                following_count TEXT DEFAULT '0',
                avatar_url TEXT DEFAULT '',
                is_monitoring BOOLEAN DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ä¸ºé«˜é¢‘æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•
        try:
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_rank ON researchers(rank);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_name ON researchers(name);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_monitoring ON researchers(is_monitoring);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_account ON researchers(x_account);')
        except Exception as e:
            logger.warning(f"åˆ›å»ºç´¢å¼•æ—¶é‡åˆ°è­¦å‘Š: {e}")
        
        # å†…å®¹è¡¨ - ä¼˜åŒ–å­˜å‚¨å’Œç´¢å¼•
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS x_content (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                researcher_id INTEGER,
                tweet_id TEXT UNIQUE,
                content TEXT,
                content_type TEXT DEFAULT 'text',
                likes_count INTEGER DEFAULT 0,
                retweets_count INTEGER DEFAULT 0,
                replies_count INTEGER DEFAULT 0,
                created_at DATETIME,
                collected_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (researcher_id) REFERENCES researchers (id) ON DELETE CASCADE
            )
        ''')
        
        # å†…å®¹è¡¨ç´¢å¼•
        try:
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_researcher ON x_content(researcher_id);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_created ON x_content(created_at);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_tweet_id ON x_content(tweet_id);')
        except Exception as e:
            logger.warning(f"åˆ›å»ºå†…å®¹è¡¨ç´¢å¼•æ—¶é‡åˆ°è­¦å‘Š: {e}")
        
        # ç›‘æ§ä»»åŠ¡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS monitoring_tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                researcher_id INTEGER,
                status TEXT DEFAULT 'active',
                last_check DATETIME DEFAULT CURRENT_TIMESTAMP,
                check_interval INTEGER DEFAULT 3600,
                FOREIGN KEY (researcher_id) REFERENCES researchers (id) ON DELETE CASCADE
            )
        ''')
        
        # ç›‘æ§ä»»åŠ¡è¡¨ç´¢å¼•
        try:
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_monitoring_researcher ON monitoring_tasks(researcher_id);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_monitoring_status ON monitoring_tasks(status);')
        except Exception as e:
            logger.warning(f"åˆ›å»ºç›‘æ§ä»»åŠ¡è¡¨ç´¢å¼•æ—¶é‡åˆ°è­¦å‘Š: {e}")
        
        conn.commit()
        conn.close()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ - å·²ä¼˜åŒ–æ”¯æŒå¤§è§„æ¨¡æ•°æ®")
    
    def load_sample_data(self):
        """åŠ è½½ç ”ç©¶è€…ç¤ºä¾‹æ•°æ® (æ­¤ä¸ºåº”ç”¨åŸºç¡€æ•°æ®ï¼ŒéåŠ¨æ€å†…å®¹)"""
        researchers_data = [
            {
                'rank': 1, 'name': 'Ilya Sutskever', 'country': 'Canada', 'company': 'SSI',
                'research_focus': 'AlexNetã€Seq2seqã€æ·±åº¦å­¦ä¹ ', 'x_account': '@ilyasut',
                'followers_count': '127K', 'following_count': '89'
            },
            {
                'rank': 2, 'name': 'Noam Shazeer', 'country': 'USA', 'company': 'Google Deepmind',
                'research_focus': 'æ³¨æ„åŠ›æœºåˆ¶ã€æ··åˆä¸“å®¶æ¨¡å‹ã€è§’è‰²AI', 'x_account': '@noamshazeer',
                'followers_count': '45K', 'following_count': '156'
            },
            {
                'rank': 3, 'name': 'Geoffrey Hinton', 'country': 'UK', 'company': 'University of Toronto',
                'research_focus': 'åå‘ä¼ æ’­ã€ç»å°”å…¹æ›¼æœºã€æ·±åº¦å­¦ä¹ ', 'x_account': '@geoffreyhinton',
                'followers_count': '234K', 'following_count': '67'
            },
            {
                'rank': 4, 'name': 'Alec Radford', 'country': 'USA', 'company': 'Thinking Machines',
                'research_focus': 'ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€GPTã€CLIP', 'x_account': '@alec_radford',
                'followers_count': '89K', 'following_count': '123'
            },
            {
                'rank': 5, 'name': 'Andrej Karpathy', 'country': 'Slovakia', 'company': 'Tesla',
                'research_focus': 'è®¡ç®—æœºè§†è§‰ã€ç¥ç»ç½‘ç»œã€è‡ªåŠ¨é©¾é©¶', 'x_account': '@karpathy',
                'followers_count': '512K', 'following_count': '234'
            }
        ]
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        for researcher in researchers_data:
            cursor.execute('''
                INSERT OR IGNORE INTO researchers 
                (rank, name, country, company, research_focus, x_account, followers_count, following_count)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                researcher['rank'], researcher['name'], researcher['country'],
                researcher['company'], researcher['research_focus'], researcher['x_account'],
                researcher['followers_count'], researcher['following_count']
            ))
        
        conn.commit()
        conn.close()

# åˆå§‹åŒ–
try:
    researcher_manager = ResearcherManager()
    logger.info("âœ… ç ”ç©¶è€…ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ ç ”ç©¶è€…ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    researcher_manager = None

try:
    twitter_api = TwitterAPI()
    logger.info("âœ… Twitter APIåˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    logger.error(f"âŒ Twitter APIåˆå§‹åŒ–å¤±è´¥: {e}")
    twitter_api = None

# ç›‘æ§ä»»åŠ¡ - ä¼˜åŒ–æ”¯æŒå¤§è§„æ¨¡ç›‘æ§
class MonitoringService:
    def __init__(self):
        self.running = False
        self.thread = None
        self.max_concurrent_checks = 10  # æœ€å¤§å¹¶å‘æ£€æŸ¥æ•°
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§æœåŠ¡"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitoring_loop, daemon=True)
            self.thread.start()
            logger.info("ğŸš€ ç›‘æ§æœåŠ¡å·²å¯åŠ¨ - æ”¯æŒå¤§è§„æ¨¡ç›‘æ§")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯ - ä¼˜åŒ–å¤„ç†å¤§é‡ç ”ç©¶è€…"""
        while self.running:
            try:
                self._check_researchers_batch()
                time.sleep(1800)  # æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(60)
    
    def _check_researchers_batch(self):
        """æ‰¹é‡æ£€æŸ¥æ­£åœ¨ç›‘æ§çš„ç ”ç©¶è€…"""
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT id, name, x_account FROM researchers WHERE is_monitoring = 1')
        researchers = cursor.fetchall()
        conn.close()
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥ {len(researchers)} ä½ç ”ç©¶è€…çš„å†…å®¹")
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…åŒæ—¶å¤„ç†è¿‡å¤šç ”ç©¶è€…
        batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ª
        for i in range(0, len(researchers), batch_size):
            batch = researchers[i:i + batch_size]
            self._process_researcher_batch(batch)
            time.sleep(5)  # æ‰¹æ¬¡é—´ä¼‘æ¯5ç§’
    
    def _process_researcher_batch(self, researchers_batch):
        """å¤„ç†ä¸€æ‰¹ç ”ç©¶è€…"""
        for researcher_id, name, x_account in researchers_batch:
            try:
                tweets = twitter_api.get_user_tweets(x_account, max_results=5)
                
                if not tweets:
                    continue

                conn = sqlite3.connect('research_platform.db')
                cursor = conn.cursor()

                new_tweets_count = 0
                for tweet in tweets:
                    cursor.execute('''
                        INSERT OR IGNORE INTO x_content 
                        (researcher_id, tweet_id, content, likes_count, retweets_count, replies_count, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        researcher_id, tweet['id'], tweet['content'],
                        tweet['likes'], tweet['retweets'], tweet['replies'],
                        tweet['created_at']
                    ))
                    if cursor.rowcount > 0:
                        new_tweets_count += 1
                
                # æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´
                cursor.execute('''
                    UPDATE monitoring_tasks SET last_check = CURRENT_TIMESTAMP 
                    WHERE researcher_id = ?
                ''', (researcher_id,))
                
                conn.commit()
                conn.close()
                
                if new_tweets_count > 0:
                    logger.info(f"âœ… {name} æ›´æ–°äº† {new_tweets_count} æ¡æ–°å†…å®¹")
                
            except Exception as e:
                logger.error(f"æ£€æŸ¥ {name} æ—¶å‡ºé”™: {e}")
                time.sleep(1)  # å‡ºé”™æ—¶ç¨ä½œç­‰å¾…

# åˆå§‹åŒ–ç›‘æ§æœåŠ¡
try:
    monitoring_service = MonitoringService()
    logger.info("âœ… ç›‘æ§æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ ç›‘æ§æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    monitoring_service = None

# APIè·¯ç”±
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/researchers')
def get_researchers():
    """è·å–ç ”ç©¶è€…åˆ—è¡¨ - æ”¯æŒåˆ†é¡µå¤„ç†å¤§é‡æ•°æ®"""
    if not researcher_manager:
        return jsonify({'error': 'System not properly initialized'}), 500
        
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    search_query = request.args.get('search', '')
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)  # é»˜è®¤æ¯é¡µ50æ¡
    
    # é™åˆ¶æ¯é¡µæœ€å¤§æ•°é‡
    per_page = min(per_page, 200)
    offset = (page - 1) * per_page
    
    if search_query:
        # æœç´¢æŸ¥è¯¢
        count_query = '''
            SELECT COUNT(*) FROM researchers 
            WHERE name LIKE ? OR company LIKE ? OR research_focus LIKE ?
        '''
        cursor.execute(count_query, (f'%{search_query}%', f'%{search_query}%', f'%{search_query}%'))
        total_count = cursor.fetchone()[0]
        
        data_query = '''
            SELECT * FROM researchers 
            WHERE name LIKE ? OR company LIKE ? OR research_focus LIKE ?
            ORDER BY rank LIMIT ? OFFSET ?
        '''
        cursor.execute(data_query, (f'%{search_query}%', f'%{search_query}%', f'%{search_query}%', per_page, offset))
    else:
        # æ™®é€šæŸ¥è¯¢
        cursor.execute('SELECT COUNT(*) FROM researchers')
        total_count = cursor.fetchone()[0]
        
        cursor.execute('SELECT * FROM researchers ORDER BY rank LIMIT ? OFFSET ?', (per_page, offset))
    
    researchers = []
    for row in cursor.fetchall():
        researchers.append({
            'id': row[0], 'rank': row[1], 'name': row[2], 'country': row[3],
            'company': row[4], 'research_focus': row[5], 'x_account': row[6],
            'followers_count': row[7], 'following_count': row[8],
            'is_monitoring': bool(row[10])
        })
    
    conn.close()
    
    return jsonify({
        'researchers': researchers,
        'pagination': {
            'page': page,
            'per_page': per_page,
            'total': total_count,
            'pages': (total_count + per_page - 1) // per_page
        }
    })

@app.route('/api/researcher/<int:researcher_id>')
def get_researcher_detail(researcher_id):
    """è·å–ç ”ç©¶è€…è¯¦æƒ…"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT * FROM researchers WHERE id = ?', (researcher_id,))
    researcher_row = cursor.fetchone()
    
    if not researcher_row:
        conn.close()
        return jsonify({'error': 'Researcher not found'}), 404
    
    researcher = {
        'id': researcher_row[0], 'rank': researcher_row[1], 'name': researcher_row[2],
        'country': researcher_row[3], 'company': researcher_row[4], 
        'research_focus': researcher_row[5], 'x_account': researcher_row[6],
        'followers_count': researcher_row[7], 'following_count': researcher_row[8],
        'is_monitoring': bool(researcher_row[10])
    }
    
    # è·å–æœ€æ–°å†…å®¹
    cursor.execute('''
        SELECT * FROM x_content WHERE researcher_id = ? 
        ORDER BY created_at DESC LIMIT 10
    ''', (researcher_id,))
    content_rows = cursor.fetchall()
    
    conn.close()
    
    recent_content = [
        {
            'id': c[0], 'content': c[3], 'likes': c[5], 
            'retweets': c[6], 'replies': c[7], 'created_at': c[8]
        } for c in content_rows
    ]
    
    return jsonify({
        'researcher': researcher,
        'recent_content': recent_content
    })

@app.route('/api/researcher/<int:researcher_id>', methods=['DELETE'])
def delete_researcher(researcher_id):
    """åˆ é™¤æŒ‡å®šçš„ç ”ç©¶è€…åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()

        cursor.execute("PRAGMA foreign_keys = ON;")
        cursor.execute('DELETE FROM researchers WHERE id = ?', (researcher_id,))
        
        conn.commit()

        if cursor.rowcount > 0:
            logger.info(f"âœ… æˆåŠŸåˆ é™¤ç ”ç©¶è€… ID: {researcher_id}")
            return jsonify({'message': f'æˆåŠŸåˆ é™¤ç ”ç©¶è€… ID: {researcher_id}'}), 200
        else:
            logger.warning(f"âš ï¸ å°è¯•åˆ é™¤ä¸€ä¸ªä¸å­˜åœ¨çš„ç ”ç©¶è€… ID: {researcher_id}")
            return jsonify({'error': 'Researcher not found'}), 404

    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ç ”ç©¶è€… {researcher_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'error': 'Internal server error'}), 500
    finally:
        if conn:
            conn.close()

@app.route('/api/content')
def get_content():
    """è·å–æ‰€æœ‰å†…å®¹ - æ”¯æŒåˆ†é¡µ"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)
    per_page = min(per_page, 100)  # é™åˆ¶æœ€å¤§æ¯é¡µæ•°é‡
    offset = (page - 1) * per_page
    
    # è·å–æ€»æ•°
    cursor.execute('SELECT COUNT(*) FROM x_content')
    total_count = cursor.fetchone()[0]
    
    query = '''
        SELECT c.id, c.content, c.content_type, c.likes_count, c.retweets_count, 
               c.replies_count, c.created_at, c.collected_at, r.name, r.x_account 
        FROM x_content c
        JOIN researchers r ON c.researcher_id = r.id
        ORDER BY c.created_at DESC
        LIMIT ? OFFSET ?
    '''
    
    cursor.execute(query, (per_page, offset))
    content_list = []
    
    for row in cursor.fetchall():
        content_list.append({
            'id': row[0],
            'content': row[1],
            'content_type': row[2],
            'likes_count': row[3],
            'retweets_count': row[4],
            'replies_count': row[5],
            'created_at': row[6],
            'collected_at': row[7],
            'author_name': row[8] or 'Unknown',
            'author_handle': row[9] or '@unknown'
        })
    
    conn.close()
    return jsonify({
        'content': content_list,
        'pagination': {
            'page': page,
            'per_page': per_page,
            'total': total_count,
            'pages': (total_count + per_page - 1) // per_page
        }
    })

@app.route('/api/start_monitoring', methods=['POST'])
def start_monitoring_route():
    """å¼€å§‹ç›‘æ§æŒ‡å®šç ”ç©¶è€… - æ”¯æŒæ‰¹é‡æ“ä½œ"""
    data = request.get_json()
    researcher_ids = data.get('researcher_ids', [])
    
    if not researcher_ids:
        return jsonify({'error': 'No researchers selected'}), 400
    
    if len(researcher_ids) > 1000:  # å•æ¬¡æœ€å¤š1000ä¸ª
        return jsonify({'error': 'Too many researchers selected at once (max: 1000)'}), 400
    
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    success_count = 0
    failed_ids = []
    
    # å¼€å§‹äº‹åŠ¡
    cursor.execute('BEGIN TRANSACTION')
    
    try:
        for researcher_id in researcher_ids:
            try:
                # æ›´æ–°ç ”ç©¶è€…ç›‘æ§çŠ¶æ€
                cursor.execute('UPDATE researchers SET is_monitoring = 1, updated_at = CURRENT_TIMESTAMP WHERE id = ?', (researcher_id,))
                
                # åˆ›å»ºç›‘æ§ä»»åŠ¡
                cursor.execute('INSERT OR REPLACE INTO monitoring_tasks (researcher_id, status, last_check) VALUES (?, \'active\', CURRENT_TIMESTAMP)', (researcher_id,))
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"å¯åŠ¨ç›‘æ§ç ”ç©¶è€… {researcher_id} å¤±è´¥: {e}")
                failed_ids.append(researcher_id)
        
        cursor.execute('COMMIT')
        
    except Exception as e:
        cursor.execute('ROLLBACK')
        logger.error(f"æ‰¹é‡å¯åŠ¨ç›‘æ§å¤±è´¥: {e}")
        return jsonify({'error': 'Failed to start monitoring'}), 500
    
    finally:
        conn.close()
    
    # ç¡®ä¿ç›‘æ§æœåŠ¡æ­£åœ¨è¿è¡Œ
    monitoring_service.start_monitoring()
    
    response_data = {
        'message': f'æˆåŠŸå¯åŠ¨ç›‘æ§ {success_count} ä½ç ”ç©¶è€…',
        'monitoring_count': success_count
    }
    
    if failed_ids:
        response_data['failed_ids'] = failed_ids
        response_data['message'] += f', {len(failed_ids)} ä½å¤±è´¥'
    
    return jsonify(response_data)

@app.route('/api/stop_monitoring', methods=['POST'])
def stop_monitoring_route():
    """åœæ­¢ç›‘æ§æŒ‡å®šç ”ç©¶è€…"""
    data = request.get_json()
    researcher_ids = data.get('researcher_ids', [])
    
    if len(researcher_ids) > 1000:
        return jsonify({'error': 'Too many researchers selected at once (max: 1000)'}), 400
    
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    cursor.execute('BEGIN TRANSACTION')
    
    try:
        for researcher_id in researcher_ids:
            cursor.execute('UPDATE researchers SET is_monitoring = 0, updated_at = CURRENT_TIMESTAMP WHERE id = ?', (researcher_id,))
            cursor.execute('UPDATE monitoring_tasks SET status = \'inactive\' WHERE researcher_id = ?', (researcher_id,))
        
        cursor.execute('COMMIT')
        
    except Exception as e:
        cursor.execute('ROLLBACK')
        logger.error(f"æ‰¹é‡åœæ­¢ç›‘æ§å¤±è´¥: {e}")
        return jsonify({'error': 'Failed to stop monitoring'}), 500
    
    finally:
        conn.close()
    
    return jsonify({'message': f'å·²åœæ­¢ç›‘æ§ {len(researcher_ids)} ä½ç ”ç©¶è€…'})

@app.route('/api/fetch_content/<int:researcher_id>', methods=['POST'])
def fetch_researcher_content(researcher_id):
    """ç«‹å³è·å–æŒ‡å®šç ”ç©¶è€…çš„æœ€æ–°å†…å®¹"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT name, x_account FROM researchers WHERE id = ?', (researcher_id,))
    researcher = cursor.fetchone()
    
    if not researcher:
        conn.close()
        return jsonify({'error': 'Researcher not found'}), 404
    
    name, x_account = researcher
    
    try:
        # è·å–æœ€æ–°æ¨æ–‡
        tweets = twitter_api.get_user_tweets(x_account, max_results=10)
        
        new_content_count = 0
        if tweets:
            for tweet in tweets:
                cursor.execute('''
                    INSERT OR IGNORE INTO x_content 
                    (researcher_id, tweet_id, content, likes_count, retweets_count, replies_count, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    researcher_id, tweet['id'], tweet['content'],
                    tweet['likes'], tweet['retweets'], tweet['replies'],
                    tweet['created_at']
                ))
                
                if cursor.rowcount > 0:
                    new_content_count += 1
            
            conn.commit()

        conn.close()
        
        message = f'æˆåŠŸè·å– {name} çš„å†…å®¹ã€‚' if tweets else f'æœªæ‰¾åˆ° {name} çš„æ–°å†…å®¹ã€‚'
        return jsonify({
            'message': message,
            'new_content_count': new_content_count,
            'total_fetched': len(tweets)
        })
        
    except Exception as e:
        conn.close()
        logger.error(f"è·å– {name} å†…å®¹å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/analytics')
def get_analytics():
    """è·å–å¹³å°åˆ†ææ•°æ®"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    # åŸºç¡€ç»Ÿè®¡
    cursor.execute('SELECT COUNT(*) FROM researchers')
    total_researchers = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM researchers WHERE is_monitoring = 1')
    monitoring_researchers = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM x_content')
    total_content = cursor.fetchone()[0]
    
    cursor.execute('SELECT SUM(likes_count + retweets_count + replies_count) FROM x_content')
    total_engagement = cursor.fetchone()[0] or 0
    
    # å›½å®¶åˆ†å¸ƒ
    cursor.execute('SELECT country, COUNT(*) FROM researchers GROUP BY country')
    country_distribution = {k: v for k, v in cursor.fetchall() if k}
    
    # å…¬å¸åˆ†å¸ƒ
    cursor.execute('SELECT company, COUNT(*) FROM researchers GROUP BY company')
    company_distribution = {k: v for k, v in cursor.fetchall() if k}
    
    # æœ€è¿‘7å¤©çš„å†…å®¹è¶‹åŠ¿
    cursor.execute('''
        SELECT DATE(created_at), COUNT(*) 
        FROM x_content 
        WHERE created_at >= date('now', '-7 days')
        GROUP BY DATE(created_at)
        ORDER BY DATE(created_at)
    ''')
    content_trend = dict(cursor.fetchall())
    
    # ç›‘æ§èƒ½åŠ›çŠ¶æ€
    cursor.execute('SELECT MAX(rank) FROM researchers')
    max_capacity = 5000  # æœ€å¤§æ”¯æŒå®¹é‡
    current_capacity = cursor.fetchone()[0] or 0
    
    conn.close()
    
    return jsonify({
        'total_researchers': total_researchers,
        'monitoring_researchers': monitoring_researchers,
        'total_content': total_content,
        'total_engagement': total_engagement,
        'country_distribution': country_distribution,
        'company_distribution': company_distribution,
        'content_trend': content_trend,
        'api_status': 'connected' if twitter_api.client else 'disconnected',
        'monitoring_active': monitoring_service.running,
        'capacity': {
            'current': total_researchers,
            'monitoring': monitoring_researchers,
            'max_supported': max_capacity,
            'utilization': f"{(total_researchers/max_capacity)*100:.1f}%"
        }
    })

@app.route('/api/upload_excel', methods=['POST'])
def upload_excel():
    """ä¸Šä¼ Excelæ–‡ä»¶ - å¢å¼ºé”™è¯¯å¤„ç†å’Œæ‰¹é‡å¯¼å…¥"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.lower().endswith(('.xlsx', '.xls')):
        return jsonify({'error': 'Please upload an Excel file (.xlsx or .xls)'}), 400
    
    try:
        import openpyxl
        workbook = openpyxl.load_workbook(file)
        worksheet = workbook.active
        
        logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†Excelæ–‡ä»¶ï¼Œå…± {worksheet.max_row - 1} è¡Œæ•°æ®")
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # å¼€å§‹äº‹åŠ¡
        cursor.execute('BEGIN TRANSACTION')
        
        added_count = 0
        error_count = 0
        skipped_count = 0
        error_details = []
        
        # æ‰¹é‡å¤„ç†æ•°æ®
        batch_size = 100
        batch_data = []
        
        for row_num, row in enumerate(worksheet.iter_rows(min_row=2, values_only=True), start=2):
            try:
                # æ•°æ®éªŒè¯
                if not row or len(row) < 6:
                    skipped_count += 1
                    logger.warning(f"ç¬¬ {row_num} è¡Œï¼šæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    continue
                
                if not row[1]:  # åå­—ä¸èƒ½ä¸ºç©º
                    skipped_count += 1
                    logger.warning(f"ç¬¬ {row_num} è¡Œï¼šç ”ç©¶è€…å§“åä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ¸…ç†å’ŒéªŒè¯æ•°æ®
                rank = row[0] if row[0] is not None else row_num - 1
                name = str(row[1]).strip() if row[1] else ''
                country = str(row[2]).strip() if row[2] else ''
                company = str(row[3]).strip() if row[3] else ''
                research_focus = str(row[4]).strip() if row[4] else ''
                x_account = str(row[5]).strip() if row[5] else ''
                
                # ç¡®ä¿ X è´¦å·æ ¼å¼æ­£ç¡®
                if x_account and not x_account.startswith('@'):
                    x_account = '@' + x_account
                
                batch_data.append((rank, name, country, company, research_focus, x_account))
                
                # è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶æ‰§è¡Œæ’å…¥
                if len(batch_data) >= batch_size:
                    added_count += insert_researcher_batch(cursor, batch_data, error_details)
                    batch_data = []
                
            except Exception as e:
                error_count += 1
                error_msg = f"ç¬¬ {row_num} è¡Œå¤„ç†å¤±è´¥: {str(e)}"
                logger.error(error_msg)
                error_details.append(error_msg)
                
                if error_count > 50:  # å¦‚æœé”™è¯¯å¤ªå¤šï¼Œåœæ­¢å¤„ç†
                    logger.error("é”™è¯¯è¿‡å¤šï¼Œåœæ­¢å¤„ç†æ–‡ä»¶")
                    break
        
        # å¤„ç†å‰©ä½™çš„æ‰¹é‡æ•°æ®
        if batch_data:
            added_count += insert_researcher_batch(cursor, batch_data, error_details)
        
        # æäº¤äº‹åŠ¡
        cursor.execute('COMMIT')
        conn.close()
        
        total_processed = worksheet.max_row - 1
        
        logger.info(f"âœ… Excelå¯¼å…¥å®Œæˆ: æˆåŠŸ {added_count}, è·³è¿‡ {skipped_count}, é”™è¯¯ {error_count}")
        
        response_data = {
            'message': f'Excelæ–‡ä»¶å¤„ç†å®Œæˆ',
            'total_rows': total_processed,
            'imported': added_count,
            'skipped': skipped_count,
            'errors': error_count
        }
        
        if error_details and len(error_details) <= 20:  # åªè¿”å›å‰20ä¸ªé”™è¯¯
            response_data['error_details'] = error_details[:20]
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"âŒ Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return jsonify({
            'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}',
            'suggestion': 'è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„åˆ—ï¼šæ’åã€å§“åã€å›½å®¶ã€å…¬å¸ã€ç ”ç©¶é¢†åŸŸã€Xè´¦å·'
        }), 500

def insert_researcher_batch(cursor, batch_data, error_details):
    """æ‰¹é‡æ’å…¥ç ”ç©¶è€…æ•°æ®"""
    added_count = 0
    
    for data in batch_data:
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO researchers 
                (rank, name, country, company, research_focus, x_account)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', data)
            added_count += 1
            
        except Exception as e:
            error_msg = f"æ’å…¥æ•°æ®å¤±è´¥ {data[1]}: {str(e)}"
            error_details.append(error_msg)
            logger.error(error_msg)
    
    return added_count

@app.route('/api/system_status')
def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    # æ•°æ®åº“ç»Ÿè®¡
    cursor.execute('SELECT COUNT(*) FROM researchers')
    total_researchers = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM researchers WHERE is_monitoring = 1')
    monitoring_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM x_content')
    total_content = cursor.fetchone()[0]
    
    # æœ€è¿‘24å°æ—¶çš„æ´»åŠ¨
    cursor.execute('''
        SELECT COUNT(*) FROM x_content 
        WHERE collected_at >= datetime('now', '-1 day')
    ''')
    recent_content = cursor.fetchone()[0]
    
    conn.close()
    
    return jsonify({
        'system_capacity': {
            'max_researchers': 5000,
            'current_researchers': total_researchers,
            'available_slots': 5000 - total_researchers,
            'utilization_percentage': (total_researchers / 5000) * 100
        },
        'monitoring_status': {
            'active_monitoring': monitoring_count,
            'max_concurrent': 1000,
            'service_running': monitoring_service.running
        },
        'data_statistics': {
            'total_content': total_content,
            'recent_24h': recent_content
        },
        'api_status': {
            'twitter_connected': twitter_api.client is not None,
            'last_check': datetime.now().isoformat()
        }
    })

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy' if researcher_manager else 'partial',
        'timestamp': datetime.now().isoformat(),
        'twitter_api': 'connected' if twitter_api and twitter_api.client else 'disconnected',
        'monitoring': 'active' if monitoring_service and monitoring_service.running else 'inactive',
        'capacity': '5000 researchers supported',
        'components': {
            'researcher_manager': 'ok' if researcher_manager else 'failed',
            'twitter_api': 'ok' if twitter_api else 'failed',
            'monitoring_service': 'ok' if monitoring_service else 'failed'
        }
    })

@app.route('/api/init_status')
def get_init_status():
    """è·å–åˆå§‹åŒ–çŠ¶æ€"""
    return jsonify({
        'initialized': bool(researcher_manager),
        'components': {
            'database': bool(researcher_manager),
            'twitter_api': bool(twitter_api),
            'monitoring': bool(monitoring_service)
        },
        'ready': bool(researcher_manager and twitter_api and monitoring_service)
    })

if __name__ == '__main__':
    logger.info("ğŸš€ AIç ”ç©¶è€…Xå†…å®¹å­¦ä¹ å¹³å°å¯åŠ¨ä¸­...")
    logger.info(f"ğŸ“Š ç³»ç»Ÿå®¹é‡: æœ€å¤§æ”¯æŒ 5000 ä½ç ”ç©¶è€…ç›‘æ§")
    logger.info(f"Twitter API: {'âœ… å·²é…ç½®' if TWITTER_BEARER_TOKEN else 'âš ï¸ æœªé…ç½®ï¼Œå°†æ— æ³•è·å–çœŸå®æ•°æ®'}")
    
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
