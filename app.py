from flask import Flask, render_template, jsonify, request, send_from_directory, send_file
import json
import tweepy
import os
import secrets
from datetime import datetime, timedelta, timezone
import sqlite3
import logging
import threading
import time

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', secrets.token_hex(32))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Twitter APIé…ç½®
TWITTER_BEARER_TOKEN = os.environ.get('TWITTER_BEARER_TOKEN')

# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
DB_FILE = 'research_platform.db'

def format_interval(seconds):
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºäººæ€§åŒ–çš„æ—¶é—´æ˜¾ç¤º"""
    if seconds < 3600:
        return f"{seconds // 60}åˆ†é’Ÿ"
    elif seconds < 86400:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        if minutes == 0:
            return f"{hours}å°æ—¶"
        else:
            return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"
    else:
        days = seconds // 86400
        hours = (seconds % 86400) // 3600
        if hours == 0:
            return f"{days}å¤©"
        else:
            return f"{days}å¤©{hours}å°æ—¶"

def insert_researcher_batch(cursor, batch_data, error_details):
    """æ‰¹é‡æ’å…¥ç ”ç©¶è€…æ•°æ®"""
    added_count = 0
    
    for data in batch_data:
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO researchers 
                (rank, name, country, company, research_focus, x_account)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', data)
            added_count += 1
            
        except Exception as e:
            error_msg = f"æ’å…¥æ•°æ®å¤±è´¥ {data[1]}: {str(e)}"
            error_details.append(error_msg)
            logger.error(error_msg)
    
    return added_count

class TwitterAPI:
    def __init__(self):
        self.client = None
        self.api_working = False
        
        if TWITTER_BEARER_TOKEN:
            try:
                self.client = tweepy.Client(
                    bearer_token=TWITTER_BEARER_TOKEN,
                    wait_on_rate_limit=True  # è‡ªåŠ¨å¤„ç†é€Ÿç‡é™åˆ¶
                )
                logger.info("âœ… Twitter APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                self.test_connection()
            except Exception as e:
                logger.error(f"âŒ Twitter APIåˆå§‹åŒ–å¤±è´¥: {e}")
                self.client = None
        else:
            self.client = None
            logger.warning("âš ï¸ Twitter Bearer Tokenæœªé…ç½®ï¼Œå°†æ— æ³•è·å–çœŸå®æ•°æ®")
    
    def test_connection(self):
        """æµ‹è¯•APIè¿æ¥"""
        try:
            if self.client:
                # æµ‹è¯•è·å–ä¸€ä¸ªç®€å•çš„ç”¨æˆ·ä¿¡æ¯
                user = self.client.get_user(username='twitter', user_fields=['public_metrics'])
                if user.data:
                    logger.info("ğŸ”— Twitter APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                    self.api_working = True
                    return True
                else:
                    logger.error("âŒ Twitter APIæµ‹è¯•å¤±è´¥ï¼šæ— æ³•è·å–ç”¨æˆ·æ•°æ®")
                    self.api_working = False
        except Exception as e:
            logger.error(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            self.api_working = False
        return False
    
    def get_user_info(self, username):
        """è·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…³æ³¨è€…æ•°é‡"""
        if not self.client:
            logger.warning(f"Twitterå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è·å– {username} çš„ç”¨æˆ·ä¿¡æ¯")
            return None
        
        if not self.api_working:
            logger.warning(f"Twitter APIæœªæ­£å¸¸å·¥ä½œï¼Œè·³è¿‡è·å– {username} çš„ç”¨æˆ·ä¿¡æ¯")
            return None
        
        try:
            # æ¸…ç†ç”¨æˆ·å
            username = username.replace('@', '').strip()
            if not username:
                logger.warning("ç”¨æˆ·åä¸ºç©º")
                return None
            
            logger.info(f"ğŸ” æ­£åœ¨è·å–ç”¨æˆ·ä¿¡æ¯: {username}")
            
            # è·å–ç”¨æˆ·ä¿¡æ¯ï¼Œå¢åŠ æ›´å¤šå­—æ®µ
            user_response = self.client.get_user(
                username=username,
                user_fields=['public_metrics', 'profile_image_url', 'description', 'verified']
            )
            
            if not user_response or not user_response.data:
                logger.warning(f"âŒ Twitterç”¨æˆ· {username} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                return None
            
            user = user_response.data
            public_metrics = getattr(user, 'public_metrics', {})
            
            user_info = {
                'id': str(user.id),
                'username': user.username,
                'name': user.name,
                'followers_count': public_metrics.get('followers_count', 0),
                'following_count': public_metrics.get('following_count', 0),
                'tweet_count': public_metrics.get('tweet_count', 0),
                'listed_count': public_metrics.get('listed_count', 0),
                'profile_image_url': getattr(user, 'profile_image_url', ''),
                'description': getattr(user, 'description', ''),
                'verified': getattr(user, 'verified', False)
            }
            
            logger.info(f"âœ… æˆåŠŸè·å– {username} çš„ä¿¡æ¯: {user_info['followers_count']} å…³æ³¨è€…, {user_info['following_count']} æ­£åœ¨å…³æ³¨")
            return user_info
            
        except tweepy.Unauthorized:
            logger.error(f"âŒ æ— æƒè®¿é—®ç”¨æˆ· {username}ï¼Œå¯èƒ½æ˜¯ç§äººè´¦æˆ·æˆ–APIæƒé™ä¸è¶³")
            return None
        except tweepy.NotFound:
            logger.error(f"âŒ ç”¨æˆ· {username} ä¸å­˜åœ¨")
            return None
        except tweepy.TooManyRequests:
            logger.error(f"âŒ APIè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•")
            return None
        except Exception as e:
            logger.error(f"âŒ è·å– {username} ç”¨æˆ·ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def get_user_tweets(self, username, max_results=10, start_time=None, end_time=None):
        """
        è·å–ç”¨æˆ·æ¨æ–‡ï¼Œæ”¯æŒè‡ªå®šä¹‰æ—¶é—´èŒƒå›´
        """
        if not self.client:
            logger.warning(f"Twitterå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è·å– {username} çš„æ¨æ–‡")
            return []
        
        if not self.api_working:
            logger.warning(f"Twitter APIæœªæ­£å¸¸å·¥ä½œï¼Œè·³è¿‡è·å– {username} çš„æ¨æ–‡")
            return []
        
        try:
            # æ¸…ç†ç”¨æˆ·å
            username = username.replace('@', '').strip()
            if not username:
                logger.warning("ç”¨æˆ·åä¸ºç©º")
                return []
            
            logger.info(f"ğŸ” æ­£åœ¨è·å–æ¨æ–‡: {username}, æœ€å¤§æ•°é‡: {max_results}")
            
            # é¦–å…ˆè·å–ç”¨æˆ·ä¿¡æ¯
            user_response = self.client.get_user(username=username)
            if not user_response or not user_response.data:
                logger.warning(f"âŒ æ— æ³•æ‰¾åˆ°ç”¨æˆ· {username}")
                return []
            
            user_id = user_response.data.id
            logger.info(f"âœ… æ‰¾åˆ°ç”¨æˆ· {username}, ID: {user_id}")
            
            # è®¾ç½®æ—¶é—´èŒƒå›´ - å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œè·å–æœ€è¿‘7å¤©çš„å†…å®¹
            if not start_time:
                start_time = datetime.now(timezone.utc) - timedelta(days=7)
            if not end_time:
                end_time = datetime.now(timezone.utc)
            
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
            
            # é™åˆ¶æœ€å¤§ç»“æœæ•°
            max_results = min(max_results, 100)
            
            # è·å–æ¨æ–‡
            tweets_response = self.client.get_users_tweets(
                id=user_id,
                max_results=max_results,
                tweet_fields=[
                    'created_at', 'public_metrics', 'context_annotations', 
                    'attachments', 'author_id', 'conversation_id'
                ],
                media_fields=['url', 'preview_image_url', 'type'],
                expansions=['attachments.media_keys'],
                exclude=['retweets', 'replies'],  # æ’é™¤è½¬å‘å’Œå›å¤
                start_time=start_time,
                end_time=end_time
            )
            
            if not tweets_response or not tweets_response.data:
                logger.info(f"â„¹ï¸ æœªæ‰¾åˆ° {username} åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ¨æ–‡")
                return []
            
            # å¤„ç†åª’ä½“ä¿¡æ¯
            media_dict = {}
            if hasattr(tweets_response, 'includes') and tweets_response.includes and 'media' in tweets_response.includes:
                for media in tweets_response.includes['media']:
                    media_dict[media.media_key] = {
                        'type': media.type,
                        'url': getattr(media, 'url', ''),
                        'preview_url': getattr(media, 'preview_image_url', '')
                    }
            
            result = []
            for tweet in tweets_response.data:
                try:
                    # å¤„ç†åª’ä½“é™„ä»¶
                    media_urls = []
                    if hasattr(tweet, 'attachments') and tweet.attachments and 'media_keys' in tweet.attachments:
                        for media_key in tweet.attachments['media_keys']:
                            if media_key in media_dict:
                                media_info = media_dict[media_key]
                                media_urls.append({
                                    'type': media_info['type'],
                                    'url': media_info['url'],
                                    'preview_url': media_info['preview_url']
                                })
                    
                    # è·å–äº’åŠ¨æ•°æ®
                    public_metrics = getattr(tweet, 'public_metrics', {})
                    
                    tweet_data = {
                        'id': str(tweet.id),
                        'content': tweet.text or '',
                        'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                        'likes': public_metrics.get('like_count', 0),
                        'retweets': public_metrics.get('retweet_count', 0),
                        'replies': public_metrics.get('reply_count', 0),
                        'quotes': public_metrics.get('quote_count', 0),
                        'author': username,
                        'type': 'text',
                        'media_urls': media_urls
                    }
                    
                    result.append(tweet_data)
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ¨æ–‡æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸè·å– {username} çš„ {len(result)} æ¡æ¨æ–‡")
            return result
            
        except tweepy.Unauthorized:
            logger.error(f"âŒ æ— æƒè®¿é—®ç”¨æˆ· {username} çš„æ¨æ–‡ï¼Œå¯èƒ½æ˜¯ç§äººè´¦æˆ·")
            return []
        except tweepy.NotFound:
            logger.error(f"âŒ ç”¨æˆ· {username} ä¸å­˜åœ¨")
            return []
        except tweepy.TooManyRequests:
            logger.error(f"âŒ APIè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•")
            return []
        except Exception as e:
            logger.error(f"âŒ è·å– {username} æ¨æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

class ResearcherManager:
    def __init__(self):
        self.init_database()
        self.load_sample_data()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ - æ”¯æŒå¤§è§„æ¨¡æ•°æ®å­˜å‚¨"""
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # å¼€å¯å¤–é”®çº¦æŸå’ŒåŸºæœ¬ä¼˜åŒ–è®¾ç½®
        cursor.execute("PRAGMA foreign_keys = ON;")
        cursor.execute("PRAGMA synchronous = NORMAL;")  # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§

        # ç ”ç©¶è€…è¡¨ - ä¼˜åŒ–å­—æ®µç±»å‹å’Œç´¢å¼•
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS researchers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                rank INTEGER,
                name TEXT NOT NULL,
                country TEXT,
                company TEXT,
                research_focus TEXT,
                x_account TEXT,
                followers_count TEXT DEFAULT '0',
                following_count TEXT DEFAULT '0',
                avatar_url TEXT DEFAULT '',
                is_monitoring BOOLEAN DEFAULT 0,
                is_special_focus BOOLEAN DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ä¸ºé«˜é¢‘æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•
        try:
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_rank ON researchers(rank);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_name ON researchers(name);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_monitoring ON researchers(is_monitoring);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_special ON researchers(is_special_focus);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_researchers_account ON researchers(x_account);')
        except Exception as e:
            logger.warning(f"åˆ›å»ºç´¢å¼•æ—¶é‡åˆ°è­¦å‘Š: {e}")
        
        # å†…å®¹è¡¨ - ä¼˜åŒ–å­˜å‚¨å’Œç´¢å¼•
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS x_content (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                researcher_id INTEGER,
                tweet_id TEXT UNIQUE,
                content TEXT,
                content_type TEXT DEFAULT 'text',
                likes_count INTEGER DEFAULT 0,
                retweets_count INTEGER DEFAULT 0,
                replies_count INTEGER DEFAULT 0,
                created_at DATETIME,
                collected_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                is_historical BOOLEAN DEFAULT 0,
                media_urls TEXT,
                FOREIGN KEY (researcher_id) REFERENCES researchers (id) ON DELETE CASCADE
            )
        ''')
        
        # å†…å®¹è¡¨ç´¢å¼•
        try:
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_researcher ON x_content(researcher_id);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_created ON x_content(created_at);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_tweet_id ON x_content(tweet_id);')
        except Exception as e:
            logger.warning(f"åˆ›å»ºå†…å®¹è¡¨ç´¢å¼•æ—¶é‡åˆ°è­¦å‘Š: {e}")
        
        # ç›‘æ§ä»»åŠ¡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS monitoring_tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                researcher_id INTEGER,
                status TEXT DEFAULT 'active',
                last_check DATETIME DEFAULT CURRENT_TIMESTAMP,
                check_interval INTEGER DEFAULT 3600,
                FOREIGN KEY (researcher_id) REFERENCES researchers (id) ON DELETE CASCADE
            )
        ''')
        
        # ç›‘æ§ä»»åŠ¡è¡¨ç´¢å¼•
        try:
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_monitoring_researcher ON monitoring_tasks(researcher_id);')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_monitoring_status ON monitoring_tasks(status);')
        except Exception as e:
            logger.warning(f"åˆ›å»ºç›‘æ§ä»»åŠ¡è¡¨ç´¢å¼•æ—¶é‡åˆ°è­¦å‘Š: {e}")

        # ç³»ç»Ÿè®¾ç½®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS system_settings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                setting_key TEXT UNIQUE NOT NULL,
                setting_value TEXT NOT NULL,
                description TEXT,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # æ’å…¥é»˜è®¤ç›‘æ§å‘¨æœŸè®¾ç½®ï¼ˆ30åˆ†é’Ÿ = 1800ç§’ï¼‰
        cursor.execute('''
            INSERT OR IGNORE INTO system_settings (setting_key, setting_value, description)
            VALUES ('monitoring_interval', '1800', 'ç›‘æ§æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰')
        ''')
        
        # åˆ›å»ºå…ƒæ•°æ®è¡¨ç”¨äºè·Ÿè¸ªæ•°æ®åº“çŠ¶æ€
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS db_metadata (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ - å·²ä¼˜åŒ–æ”¯æŒå¤§è§„æ¨¡æ•°æ®")
    
    def load_sample_data(self):
        """åŠ è½½ç ”ç©¶è€…ç¤ºä¾‹æ•°æ® (æ­¤ä¸ºåº”ç”¨åŸºç¡€æ•°æ®ï¼ŒéåŠ¨æ€å†…å®¹)"""
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½è¿‡ç¤ºä¾‹æ•°æ®
        cursor.execute('SELECT value FROM db_metadata WHERE key = ?', ('sample_data_loaded',))
        if cursor.fetchone():
            conn.close()
            return
        
        researchers_data = [
            {
                'rank': 1, 'name': 'Ilya Sutskever', 'country': 'Canada', 'company': 'SSI',
                'research_focus': 'AlexNetã€Seq2seqã€æ·±åº¦å­¦ä¹ ', 'x_account': '@ilyasut',
                'followers_count': '127K', 'following_count': '89'
            },
            {
                'rank': 2, 'name': 'Noam Shazeer', 'country': 'USA', 'company': 'Google Deepmind',
                'research_focus': 'æ³¨æ„åŠ›æœºåˆ¶ã€æ··åˆä¸“å®¶æ¨¡å‹ã€è§’è‰²AI', 'x_account': '@noamshazeer',
                'followers_count': '45K', 'following_count': '156'
            },
            {
                'rank': 3, 'name': 'Geoffrey Hinton', 'country': 'UK', 'company': 'University of Toronto',
                'research_focus': 'åå‘ä¼ æ’­ã€ç»å°”å…¹æ›¼æœºã€æ·±åº¦å­¦ä¹ ', 'x_account': '@geoffreyhinton',
                'followers_count': '234K', 'following_count': '67'
            },
            {
                'rank': 4, 'name': 'Alec Radford', 'country': 'USA', 'company': 'Thinking Machines',
                'research_focus': 'ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€GPTã€CLIP', 'x_account': '@alec_radford',
                'followers_count': '89K', 'following_count': '123'
            },
            {
                'rank': 5, 'name': 'Andrej Karpathy', 'country': 'Slovakia', 'company': 'Tesla',
                'research_focus': 'è®¡ç®—æœºè§†è§‰ã€ç¥ç»ç½‘ç»œã€è‡ªåŠ¨é©¾é©¶', 'x_account': '@karpathy',
                'followers_count': '512K', 'following_count': '234'
            }
        ]
        
        for researcher in researchers_data:
            cursor.execute('''
                INSERT OR IGNORE INTO researchers 
                (rank, name, country, company, research_focus, x_account, followers_count, following_count)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                researcher['rank'], researcher['name'], researcher['country'],
                researcher['company'], researcher['research_focus'], researcher['x_account'],
                researcher['followers_count'], researcher['following_count']
            ))
        
        # æ ‡è®°ç¤ºä¾‹æ•°æ®å·²åŠ è½½
        cursor.execute('INSERT OR REPLACE INTO db_metadata (key, value) VALUES (?, ?)', 
                      ('sample_data_loaded', 'true'))
        
        conn.commit()
        conn.close()
        logger.info("âœ… ç¤ºä¾‹æ•°æ®åŠ è½½å®Œæˆ")
        
    def load_sample_data_if_empty(self):
        """ä»…åœ¨æ•°æ®åº“ä¸ºç©ºæ—¶åŠ è½½ç¤ºä¾‹æ•°æ®"""
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM researchers')
        count = cursor.fetchone()[0]
        
        if count == 0:
            # é‡ç½®åŠ è½½æ ‡è®°
            cursor.execute('DELETE FROM db_metadata WHERE key = ?', ('sample_data_loaded',))
            conn.commit()
            conn.close()
            
            # é‡æ–°åŠ è½½ç¤ºä¾‹æ•°æ®
            self.load_sample_data()
        else:
            conn.close()

# ç›‘æ§ä»»åŠ¡ - ä¼˜åŒ–æ”¯æŒå¤§è§„æ¨¡ç›‘æ§
class MonitoringService:
    def __init__(self):
        self.running = False
        self.thread = None
        self.max_concurrent_checks = 10  # æœ€å¤§å¹¶å‘æ£€æŸ¥æ•°
        self.current_interval = self.get_monitoring_interval()  # ä»æ•°æ®åº“è·å–é—´éš”
    
    def get_monitoring_interval(self):
        """ä»æ•°æ®åº“è·å–ç›‘æ§é—´éš”è®¾ç½®"""
        try:
            conn = sqlite3.connect('research_platform.db')
            cursor = conn.cursor()
            cursor.execute('SELECT setting_value FROM system_settings WHERE setting_key = ?', ('monitoring_interval',))
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return int(result[0])
            else:
                return 1800  # é»˜è®¤30åˆ†é’Ÿ
        except Exception as e:
            logger.error(f"è·å–ç›‘æ§é—´éš”è®¾ç½®å¤±è´¥: {e}")
            return 1800
    
    def update_monitoring_interval(self, interval_seconds):
        """æ›´æ–°ç›‘æ§é—´éš”è®¾ç½®"""
        try:
            conn = sqlite3.connect('research_platform.db')
            cursor = conn.cursor()
            cursor.execute('''
                UPDATE system_settings 
                SET setting_value = ?, updated_at = CURRENT_TIMESTAMP 
                WHERE setting_key = ?
            ''', (str(interval_seconds), 'monitoring_interval'))
            conn.commit()
            conn.close()
            
            self.current_interval = interval_seconds
            logger.info(f"âœ… ç›‘æ§é—´éš”å·²æ›´æ–°ä¸º {interval_seconds} ç§’")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ç›‘æ§é—´éš”è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§æœåŠ¡"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitoring_loop, daemon=True)
            self.thread.start()
            logger.info(f"ğŸš€ ç›‘æ§æœåŠ¡å·²å¯åŠ¨ - æ”¯æŒå¤§è§„æ¨¡ç›‘æ§ï¼Œæ£€æŸ¥é—´éš”: {self.current_interval}ç§’")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯ - ä½¿ç”¨å¯é…ç½®çš„æ—¶é—´é—´éš”"""
        while self.running:
            try:
                self._check_researchers_batch()
                # ä½¿ç”¨å½“å‰è®¾ç½®çš„é—´éš”æ—¶é—´
                time.sleep(self.current_interval)
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(60)  # å‡ºé”™æ—¶ç­‰å¾…1åˆ†é’Ÿåé‡è¯•
    
    def _check_researchers_batch(self):
        """æ‰¹é‡æ£€æŸ¥æ­£åœ¨ç›‘æ§çš„ç ”ç©¶è€…"""
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT id, name, x_account FROM researchers WHERE is_monitoring = 1')
        researchers = cursor.fetchall()
        conn.close()
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥ {len(researchers)} ä½ç ”ç©¶è€…çš„å†…å®¹")
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…åŒæ—¶å¤„ç†è¿‡å¤šç ”ç©¶è€…
        batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ª
        for i in range(0, len(researchers), batch_size):
            batch = researchers[i:i + batch_size]
            self._process_researcher_batch(batch)
            time.sleep(5)  # æ‰¹æ¬¡é—´ä¼‘æ¯5ç§’
    
    def _process_researcher_batch(self, researchers_batch):
        """å¤„ç†ä¸€æ‰¹ç ”ç©¶è€…"""
        for researcher_id, name, x_account in researchers_batch:
            try:
                tweets = twitter_api.get_user_tweets(x_account, max_results=5)
                
                if not tweets:
                    continue

                conn = sqlite3.connect('research_platform.db')
                cursor = conn.cursor()

                new_tweets_count = 0
                for tweet in tweets:
                    cursor.execute('''
                        INSERT OR IGNORE INTO x_content 
                        (researcher_id, tweet_id, content, likes_count, retweets_count, replies_count, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        researcher_id, tweet['id'], tweet['content'],
                        tweet['likes'], tweet['retweets'], tweet['replies'],
                        tweet['created_at']
                    ))
                    if cursor.rowcount > 0:
                        new_tweets_count += 1
                
                # æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´
                cursor.execute('''
                    UPDATE monitoring_tasks SET last_check = CURRENT_TIMESTAMP 
                    WHERE researcher_id = ?
                ''', (researcher_id,))
                
                conn.commit()
                conn.close()
                
                if new_tweets_count > 0:
                    logger.info(f"âœ… {name} æ›´æ–°äº† {new_tweets_count} æ¡æ–°å†…å®¹")
                
            except Exception as e:
                logger.error(f"æ£€æŸ¥ {name} æ—¶å‡ºé”™: {e}")
                time.sleep(1)  # å‡ºé”™æ—¶ç¨ä½œç­‰å¾…

# åˆå§‹åŒ–
try:
    researcher_manager = ResearcherManager()
    logger.info("âœ… ç ”ç©¶è€…ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ ç ”ç©¶è€…ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    researcher_manager = None

try:
    twitter_api = TwitterAPI()
    logger.info("âœ… Twitter APIåˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    logger.error(f"âŒ Twitter APIåˆå§‹åŒ–å¤±è´¥: {e}")
    twitter_api = None

# åˆå§‹åŒ–ç›‘æ§æœåŠ¡
try:
    monitoring_service = MonitoringService()
    logger.info("âœ… ç›‘æ§æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ ç›‘æ§æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
    monitoring_service = None

# APIè·¯ç”±
@app.route('/')
def index():
    """ä¸»é¡µè·¯ç”± - è¿”å›HTMLæ¨¡æ¿"""
    try:
        return render_template('index.html')
    except Exception as e:
        logger.error(f"ä¸»é¡µåŠ è½½å¤±è´¥: {e}")
        return f"æ¨¡æ¿åŠ è½½å¤±è´¥: {str(e)}<br>è¯·ç¡®ä¿ templates/index.html æ–‡ä»¶å­˜åœ¨", 500

@app.route('/api/researchers')
def get_researchers():
    """è·å–ç ”ç©¶è€…åˆ—è¡¨ - æ”¯æŒåˆ†é¡µå¤„ç†å¤§é‡æ•°æ®"""
    if not researcher_manager:
        return jsonify({'error': 'System not properly initialized'}), 500
        
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    search_query = request.args.get('search', '')
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)
    
    # é™åˆ¶æ¯é¡µæœ€å¤§æ•°é‡
    per_page = min(per_page, 200)
    offset = (page - 1) * per_page
    
    try:
        if search_query:
            # æœç´¢æŸ¥è¯¢
            count_query = '''
                SELECT COUNT(*) FROM researchers 
                WHERE name LIKE ? OR company LIKE ? OR research_focus LIKE ?
            '''
            cursor.execute(count_query, (f'%{search_query}%', f'%{search_query}%', f'%{search_query}%'))
            total_count = cursor.fetchone()[0]
            
            data_query = '''
                SELECT * FROM researchers 
                WHERE name LIKE ? OR company LIKE ? OR research_focus LIKE ?
                ORDER BY rank LIMIT ? OFFSET ?
            '''
            cursor.execute(data_query, (f'%{search_query}%', f'%{search_query}%', f'%{search_query}%', per_page, offset))
        else:
            # æ™®é€šæŸ¥è¯¢
            cursor.execute('SELECT COUNT(*) FROM researchers')
            total_count = cursor.fetchone()[0]
            
            cursor.execute('SELECT * FROM researchers ORDER BY rank LIMIT ? OFFSET ?', (per_page, offset))
        
        researchers = []
        for row in cursor.fetchall():
            researchers.append({
                'id': row[0], 'rank': row[1], 'name': row[2], 'country': row[3],
                'company': row[4], 'research_focus': row[5], 'x_account': row[6],
                'followers_count': row[7], 'following_count': row[8],
                'is_monitoring': bool(row[10]), 'is_special_focus': bool(row[11])
            })
        
        conn.close()
        
        return jsonify({
            'researchers': researchers,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total_count,
                'pages': (total_count + per_page - 1) // per_page
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–ç ”ç©¶è€…åˆ—è¡¨å¤±è´¥: {e}")
        conn.close()
        return jsonify({'error': str(e)}), 500

@app.route('/api/researcher/<int:researcher_id>')
def get_researcher_detail(researcher_id):
    """è·å–ç ”ç©¶è€…è¯¦æƒ…"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT * FROM researchers WHERE id = ?', (researcher_id,))
    researcher_row = cursor.fetchone()
    
    if not researcher_row:
        conn.close()
        return jsonify({'error': 'Researcher not found'}), 404
    
    researcher = {
        'id': researcher_row[0], 'rank': researcher_row[1], 'name': researcher_row[2],
        'country': researcher_row[3], 'company': researcher_row[4], 
        'research_focus': researcher_row[5], 'x_account': researcher_row[6],
        'followers_count': researcher_row[7], 'following_count': researcher_row[8],
        'is_monitoring': bool(researcher_row[10]), 'is_special_focus': bool(researcher_row[11])
    }
    
    # è·å–æœ€æ–°å†…å®¹
    cursor.execute('''
        SELECT * FROM x_content WHERE researcher_id = ? 
        ORDER BY created_at DESC LIMIT 10
    ''', (researcher_id,))
    content_rows = cursor.fetchall()
    
    conn.close()
    
    recent_content = [
        {
            'id': c[0], 'content': c[3], 'likes': c[5], 
            'retweets': c[6], 'replies': c[7], 'created_at': c[8]
        } for c in content_rows
    ]
    
    return jsonify({
        'researcher': researcher,
        'recent_content': recent_content
    })

@app.route('/api/researcher/<int:researcher_id>', methods=['DELETE'])
def delete_researcher(researcher_id):
    """åˆ é™¤æŒ‡å®šçš„ç ”ç©¶è€…åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()

        cursor.execute("PRAGMA foreign_keys = ON;")
        cursor.execute('DELETE FROM researchers WHERE id = ?', (researcher_id,))
        
        conn.commit()

        if cursor.rowcount > 0:
            logger.info(f"âœ… æˆåŠŸåˆ é™¤ç ”ç©¶è€… ID: {researcher_id}")
            return jsonify({'message': f'æˆåŠŸåˆ é™¤ç ”ç©¶è€… ID: {researcher_id}'}), 200
        else:
            logger.warning(f"âš ï¸ å°è¯•åˆ é™¤ä¸€ä¸ªä¸å­˜åœ¨çš„ç ”ç©¶è€… ID: {researcher_id}")
            return jsonify({'error': 'Researcher not found'}), 404

    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ç ”ç©¶è€… {researcher_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'error': 'Internal server error'}), 500
    finally:
        if conn:
            conn.close()

@app.route('/api/content')
def get_content():
    """è·å–æ‰€æœ‰å†…å®¹ - æ”¯æŒåˆ†é¡µ"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)
    per_page = min(per_page, 100)  # é™åˆ¶æœ€å¤§æ¯é¡µæ•°é‡
    offset = (page - 1) * per_page
    
    try:
        # è·å–æ€»æ•°
        cursor.execute('SELECT COUNT(*) FROM x_content')
        total_count = cursor.fetchone()[0]
        
        query = '''
            SELECT c.id, c.content, c.content_type, c.likes_count, c.retweets_count, 
                   c.replies_count, c.created_at, c.collected_at, r.name, r.x_account 
            FROM x_content c
            JOIN researchers r ON c.researcher_id = r.id
            ORDER BY c.created_at DESC
            LIMIT ? OFFSET ?
        '''
        
        cursor.execute(query, (per_page, offset))
        content_list = []
        
        for row in cursor.fetchall():
            content_list.append({
                'id': row[0],
                'content': row[1],
                'content_type': row[2],
                'likes_count': row[3],
                'retweets_count': row[4],
                'replies_count': row[5],
                'created_at': row[6],
                'collected_at': row[7],
                'author_name': row[8] or 'Unknown',
                'author_handle': row[9] or '@unknown'
            })
        
        conn.close()
        
        # å¦‚æœæ˜¯ç®€å•è¯·æ±‚ï¼ˆæ— åˆ†é¡µå‚æ•°ï¼‰ï¼Œè¿”å›ç®€å•æ ¼å¼
        if page == 1 and per_page == 20:
            return jsonify(content_list)
            
        return jsonify({
            'content': content_list,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total_count,
                'pages': (total_count + per_page - 1) // per_page
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–å†…å®¹åˆ—è¡¨å¤±è´¥: {e}")
        conn.close()
        return jsonify({'error': str(e)}), 500

@app.route('/api/start_monitoring', methods=['POST'])
def start_monitoring_route():
    """å¼€å§‹ç›‘æ§æŒ‡å®šç ”ç©¶è€… - æ”¯æŒæ‰¹é‡æ“ä½œ"""
    data = request.get_json()
    researcher_ids = data.get('researcher_ids', [])
    
    if not researcher_ids:
        return jsonify({'error': 'No researchers selected'}), 400
    
    if len(researcher_ids) > 1000:  # å•æ¬¡æœ€å¤š1000ä¸ª
        return jsonify({'error': 'Too many researchers selected at once (max: 1000)'}), 400
    
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    success_count = 0
    failed_ids = []
    
    # å¼€å§‹äº‹åŠ¡
    cursor.execute('BEGIN TRANSACTION')
    
    try:
        for researcher_id in researcher_ids:
            try:
                # æ›´æ–°ç ”ç©¶è€…ç›‘æ§çŠ¶æ€
                cursor.execute('UPDATE researchers SET is_monitoring = 1, updated_at = CURRENT_TIMESTAMP WHERE id = ?', (researcher_id,))
                
                # åˆ›å»ºç›‘æ§ä»»åŠ¡
                cursor.execute('INSERT OR REPLACE INTO monitoring_tasks (researcher_id, status, last_check) VALUES (?, \'active\', CURRENT_TIMESTAMP)', (researcher_id,))
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"å¯åŠ¨ç›‘æ§ç ”ç©¶è€… {researcher_id} å¤±è´¥: {e}")
                failed_ids.append(researcher_id)
        
        cursor.execute('COMMIT')
        
    except Exception as e:
        cursor.execute('ROLLBACK')
        logger.error(f"æ‰¹é‡å¯åŠ¨ç›‘æ§å¤±è´¥: {e}")
        return jsonify({'error': 'Failed to start monitoring'}), 500
    
    finally:
        conn.close()
    
    # ç¡®ä¿ç›‘æ§æœåŠ¡æ­£åœ¨è¿è¡Œ
    if monitoring_service:
        monitoring_service.start_monitoring()
    
    response_data = {
        'message': f'æˆåŠŸå¯åŠ¨ç›‘æ§ {success_count} ä½ç ”ç©¶è€…',
        'monitoring_count': success_count
    }
    
    if failed_ids:
        response_data['failed_ids'] = failed_ids
        response_data['message'] += f', {len(failed_ids)} ä½å¤±è´¥'
    
    return jsonify(response_data)

@app.route('/api/stop_monitoring', methods=['POST'])
def stop_monitoring_route():
    """åœæ­¢ç›‘æ§æŒ‡å®šç ”ç©¶è€…"""
    data = request.get_json()
    researcher_ids = data.get('researcher_ids', [])
    
    if len(researcher_ids) > 1000:
        return jsonify({'error': 'Too many researchers selected at once (max: 1000)'}), 400
    
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    cursor.execute('BEGIN TRANSACTION')
    
    try:
        for researcher_id in researcher_ids:
            cursor.execute('UPDATE researchers SET is_monitoring = 0, updated_at = CURRENT_TIMESTAMP WHERE id = ?', (researcher_id,))
            cursor.execute('UPDATE monitoring_tasks SET status = \'inactive\' WHERE researcher_id = ?', (researcher_id,))
        
        cursor.execute('COMMIT')
        
    except Exception as e:
        cursor.execute('ROLLBACK')
        logger.error(f"æ‰¹é‡åœæ­¢ç›‘æ§å¤±è´¥: {e}")
        return jsonify({'error': 'Failed to stop monitoring'}), 500
    
    finally:
        conn.close()
    
    return jsonify({'message': f'å·²åœæ­¢ç›‘æ§ {len(researcher_ids)} ä½ç ”ç©¶è€…'})

@app.route('/api/fetch_content/<int:researcher_id>', methods=['POST'])
def fetch_researcher_content(researcher_id):
    """ç«‹å³è·å–æŒ‡å®šç ”ç©¶è€…çš„æœ€æ–°å†…å®¹"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    cursor.execute('SELECT name, x_account FROM researchers WHERE id = ?', (researcher_id,))
    researcher = cursor.fetchone()
    
    if not researcher:
        conn.close()
        return jsonify({'error': 'Researcher not found'}), 404
    
    name, x_account = researcher
    
    try:
        # è·å–æœ€æ–°æ¨æ–‡
        tweets = twitter_api.get_user_tweets(x_account, max_results=10) if twitter_api else []
        
        new_content_count = 0
        if tweets:
            for tweet in tweets:
                cursor.execute('''
                    INSERT OR IGNORE INTO x_content 
                    (researcher_id, tweet_id, content, likes_count, retweets_count, replies_count, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    researcher_id, tweet['id'], tweet['content'],
                    tweet['likes'], tweet['retweets'], tweet['replies'],
                    tweet['created_at']
                ))
                
                if cursor.rowcount > 0:
                    new_content_count += 1
            
            conn.commit()

        conn.close()
        
        message = f'æˆåŠŸè·å– {name} çš„å†…å®¹ã€‚' if tweets else f'æœªæ‰¾åˆ° {name} çš„æ–°å†…å®¹ã€‚'
        return jsonify({
            'message': message,
            'new_content_count': new_content_count,
            'total_fetched': len(tweets)
        })
        
    except Exception as e:
        conn.close()
        logger.error(f"è·å– {name} å†…å®¹å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/monitoring_settings')
def get_monitoring_settings():
    """è·å–ç›‘æ§è®¾ç½®"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT setting_key, setting_value, description FROM system_settings')
        settings = {}
        for row in cursor.fetchall():
            settings[row[0]] = {
                'value': row[1],
                'description': row[2]
            }
        
        conn.close()
        
        # è®¡ç®—å½“å‰é—´éš”çš„äººæ€§åŒ–æ˜¾ç¤º
        interval_seconds = int(settings.get('monitoring_interval', {}).get('value', 1800))
        interval_display = format_interval(interval_seconds)
        
        return jsonify({
            'monitoring_interval': interval_seconds,
            'interval_display': interval_display,
            'settings': settings,
            'predefined_intervals': [
                {'value': 1800, 'label': '30åˆ†é’Ÿ', 'description': 'é«˜é¢‘ç›‘æ§ï¼Œé€‚åˆçƒ­ç‚¹å…³æ³¨'},
                {'value': 3600, 'label': '1å°æ—¶', 'description': 'æ ‡å‡†ç›‘æ§ï¼Œå¹³è¡¡æ•ˆç‡ä¸æ—¶æ•ˆ'},
                {'value': 7200, 'label': '2å°æ—¶', 'description': 'ä¸­ç­‰é¢‘ç‡ï¼ŒèŠ‚çœèµ„æº'},
                {'value': 21600, 'label': '6å°æ—¶', 'description': 'ä½é¢‘ç›‘æ§ï¼Œé€‚åˆé•¿æœŸè§‚å¯Ÿ'},
                {'value': 43200, 'label': '12å°æ—¶', 'description': 'æ¯æ—¥ä¸¤æ¬¡æ£€æŸ¥'},
                {'value': 86400, 'label': '24å°æ—¶', 'description': 'æ¯æ—¥ä¸€æ¬¡ï¼Œæœ€èŠ‚çœèµ„æº'}
            ]
        })
        
    except Exception as e:
        logger.error(f"è·å–ç›‘æ§è®¾ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/monitoring_settings', methods=['POST'])
def update_monitoring_settings():
    """æ›´æ–°ç›‘æ§è®¾ç½®"""
    try:
        data = request.get_json()
        interval_seconds = data.get('monitoring_interval')
        
        if not interval_seconds or not isinstance(interval_seconds, int):
            return jsonify({'error': 'æ— æ•ˆçš„ç›‘æ§é—´éš”å€¼'}), 400
        
        if interval_seconds < 300:  # æœ€å°5åˆ†é’Ÿ
            return jsonify({'error': 'ç›‘æ§é—´éš”ä¸èƒ½å°‘äº5åˆ†é’Ÿï¼ˆ300ç§’ï¼‰'}), 400
        
        if interval_seconds > 604800:  # æœ€å¤§7å¤©
            return jsonify({'error': 'ç›‘æ§é—´éš”ä¸èƒ½è¶…è¿‡7å¤©ï¼ˆ604800ç§’ï¼‰'}), 400
        
        # æ›´æ–°ç›‘æ§æœåŠ¡çš„é—´éš”
        if monitoring_service and monitoring_service.update_monitoring_interval(interval_seconds):
            return jsonify({
                'message': f'ç›‘æ§é—´éš”å·²æ›´æ–°ä¸º {format_interval(interval_seconds)}',
                'new_interval': interval_seconds,
                'interval_display': format_interval(interval_seconds)
            })
        else:
            return jsonify({'error': 'æ›´æ–°ç›‘æ§é—´éš”å¤±è´¥'}), 500
            
    except Exception as e:
        logger.error(f"æ›´æ–°ç›‘æ§è®¾ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/analytics')
def get_analytics():
    """è·å–å¹³å°åˆ†ææ•°æ®"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    # åŸºç¡€ç»Ÿè®¡
    cursor.execute('SELECT COUNT(*) FROM researchers')
    total_researchers = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM researchers WHERE is_monitoring = 1')
    monitoring_researchers = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM x_content')
    total_content = cursor.fetchone()[0]
    
    cursor.execute('SELECT SUM(likes_count + retweets_count + replies_count) FROM x_content')
    total_engagement = cursor.fetchone()[0] or 0
    
    # å›½å®¶åˆ†å¸ƒ
    cursor.execute('SELECT country, COUNT(*) FROM researchers GROUP BY country')
    country_distribution = {k: v for k, v in cursor.fetchall() if k}
    
    # å…¬å¸åˆ†å¸ƒ
    cursor.execute('SELECT company, COUNT(*) FROM researchers GROUP BY company')
    company_distribution = {k: v for k, v in cursor.fetchall() if k}
    
    # æœ€è¿‘7å¤©çš„å†…å®¹è¶‹åŠ¿
    cursor.execute('''
        SELECT DATE(created_at), COUNT(*) 
        FROM x_content 
        WHERE created_at >= date('now', '-7 days')
        GROUP BY DATE(created_at)
        ORDER BY DATE(created_at)
    ''')
    content_trend = dict(cursor.fetchall())
    
    # ç›‘æ§èƒ½åŠ›çŠ¶æ€
    cursor.execute('SELECT MAX(rank) FROM researchers')
    max_capacity = 5000  # æœ€å¤§æ”¯æŒå®¹é‡
    current_capacity = cursor.fetchone()[0] or 0
    
    # è·å–ç›‘æ§é—´éš”è®¾ç½®
    monitoring_interval = 1800  # é»˜è®¤å€¼
    interval_display = "30åˆ†é’Ÿ"
    try:
        cursor.execute('SELECT setting_value FROM system_settings WHERE setting_key = ?', ('monitoring_interval',))
        result = cursor.fetchone()
        if result:
            monitoring_interval = int(result[0])
            interval_display = format_interval(monitoring_interval)
    except Exception as e:
        logger.error(f"è·å–ç›‘æ§é—´éš”è®¾ç½®å¤±è´¥: {e}")
    
    conn.close()
    
    return jsonify({
        'total_researchers': total_researchers,
        'monitoring_researchers': monitoring_researchers,
        'total_content': total_content,
        'total_engagement': total_engagement,
        'country_distribution': country_distribution,
        'company_distribution': company_distribution,
        'content_trend': content_trend,
        'api_status': 'connected' if twitter_api and twitter_api.client else 'disconnected',
        'monitoring_active': monitoring_service and monitoring_service.running,
        'monitoring_interval': monitoring_interval,
        'interval_display': interval_display,
        'capacity': {
            'current': total_researchers,
            'monitoring': monitoring_researchers,
            'max_supported': max_capacity,
            'utilization': f"{(total_researchers/max_capacity)*100:.1f}%"
        }
    })

@app.route('/api/upload_excel', methods=['POST'])
def upload_excel():
    """ä¸Šä¼ Excelæ–‡ä»¶ - å¢å¼ºé”™è¯¯å¤„ç†å’Œæ‰¹é‡å¯¼å…¥"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.lower().endswith(('.xlsx', '.xls')):
        return jsonify({'error': 'Please upload an Excel file (.xlsx or .xls)'}), 400
    
    try:
        import openpyxl
        workbook = openpyxl.load_workbook(file)
        worksheet = workbook.active
        
        logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†Excelæ–‡ä»¶ï¼Œå…± {worksheet.max_row - 1} è¡Œæ•°æ®")
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # å¼€å§‹äº‹åŠ¡
        cursor.execute('BEGIN TRANSACTION')
        
        added_count = 0
        error_count = 0
        skipped_count = 0
        error_details = []
        
        # æ‰¹é‡å¤„ç†æ•°æ®
        batch_size = 100
        batch_data = []
        
        for row_num, row in enumerate(worksheet.iter_rows(min_row=2, values_only=True), start=2):
            try:
                # æ•°æ®éªŒè¯
                if not row or len(row) < 6:
                    skipped_count += 1
                    logger.warning(f"ç¬¬ {row_num} è¡Œï¼šæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    continue
                
                if not row[1]:  # åå­—ä¸èƒ½ä¸ºç©º
                    skipped_count += 1
                    logger.warning(f"ç¬¬ {row_num} è¡Œï¼šç ”ç©¶è€…å§“åä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ¸…ç†å’ŒéªŒè¯æ•°æ®
                rank = row[0] if row[0] is not None else row_num - 1
                name = str(row[1]).strip() if row[1] else ''
                country = str(row[2]).strip() if row[2] else ''
                company = str(row[3]).strip() if row[3] else ''
                research_focus = str(row[4]).strip() if row[4] else ''
                x_account = str(row[5]).strip() if row[5] else ''
                
                # ç¡®ä¿ X è´¦å·æ ¼å¼æ­£ç¡®
                if x_account and not x_account.startswith('@'):
                    x_account = '@' + x_account
                
                batch_data.append((rank, name, country, company, research_focus, x_account))
                
                # è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶æ‰§è¡Œæ’å…¥
                if len(batch_data) >= batch_size:
                    added_count += insert_researcher_batch(cursor, batch_data, error_details)
                    batch_data = []
                
            except Exception as e:
                error_count += 1
                error_msg = f"ç¬¬ {row_num} è¡Œå¤„ç†å¤±è´¥: {str(e)}"
                logger.error(error_msg)
                error_details.append(error_msg)
                
                if error_count > 50:  # å¦‚æœé”™è¯¯å¤ªå¤šï¼Œåœæ­¢å¤„ç†
                    logger.error("é”™è¯¯è¿‡å¤šï¼Œåœæ­¢å¤„ç†æ–‡ä»¶")
                    break
        
        # å¤„ç†å‰©ä½™çš„æ‰¹é‡æ•°æ®
        if batch_data:
            added_count += insert_researcher_batch(cursor, batch_data, error_details)
        
        # æäº¤äº‹åŠ¡
        cursor.execute('COMMIT')
        conn.close()
        
        total_processed = worksheet.max_row - 1
        
        logger.info(f"âœ… Excelå¯¼å…¥å®Œæˆ: æˆåŠŸ {added_count}, è·³è¿‡ {skipped_count}, é”™è¯¯ {error_count}")
        
        response_data = {
            'message': f'Excelæ–‡ä»¶å¤„ç†å®Œæˆ',
            'total_rows': total_processed,
            'imported': added_count,
            'skipped': skipped_count,
            'errors': error_count
        }
        
        if error_details and len(error_details) <= 20:  # åªè¿”å›å‰20ä¸ªé”™è¯¯
            response_data['error_details'] = error_details[:20]
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"âŒ Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return jsonify({
            'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}',
            'suggestion': 'è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„åˆ—ï¼šæ’åã€å§“åã€å›½å®¶ã€å…¬å¸ã€ç ”ç©¶é¢†åŸŸã€Xè´¦å·'
        }), 500

@app.route('/api/special_focus', methods=['POST'])
def set_special_focus():
    """è®¾ç½®ç‰¹åˆ«å…³æ³¨"""
    try:
        data = request.get_json()
        researcher_ids = data.get('researcher_ids', [])
        is_special = data.get('is_special', True)
        
        if not researcher_ids:
            return jsonify({'error': 'No researchers selected'}), 400
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        success_count = 0
        for researcher_id in researcher_ids:
            cursor.execute('''
                UPDATE researchers 
                SET is_special_focus = ?, updated_at = CURRENT_TIMESTAMP 
                WHERE id = ?
            ''', (is_special, researcher_id))
            success_count += 1
        
        conn.commit()
        conn.close()
        
        action = "è®¾ä¸ºç‰¹åˆ«å…³æ³¨" if is_special else "å–æ¶ˆç‰¹åˆ«å…³æ³¨"
        return jsonify({
            'message': f'æˆåŠŸ{action} {success_count} ä½ç ”ç©¶è€…',
            'success_count': success_count
        })
        
    except Exception as e:
        logger.error(f"è®¾ç½®ç‰¹åˆ«å…³æ³¨å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/special_focus')
def get_special_focus():
    """è·å–ç‰¹åˆ«å…³æ³¨çš„ç ”ç©¶è€…åˆ—è¡¨"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, rank, name, country, company, research_focus, x_account, 
                   followers_count, following_count, avatar_url, is_monitoring, is_special_focus
            FROM researchers 
            WHERE is_special_focus = 1 
            ORDER BY name
        ''')
        
        researchers = []
        for row in cursor.fetchall():
            researchers.append({
                'id': row[0], 'rank': row[1], 'name': row[2], 'country': row[3],
                'company': row[4], 'research_focus': row[5], 'x_account': row[6],
                'followers_count': row[7], 'following_count': row[8],
                'avatar_url': row[9], 'is_monitoring': bool(row[10]), 
                'is_special_focus': bool(row[11])
            })
        
        conn.close()
        logger.info(f"è·å–ç‰¹åˆ«å…³æ³¨åˆ—è¡¨æˆåŠŸï¼Œå…± {len(researchers)} ä½")
        return jsonify(researchers)
        
    except Exception as e:
        logger.error(f"è·å–ç‰¹åˆ«å…³æ³¨åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/update_user_info/<int:researcher_id>', methods=['POST'])
def update_user_info(researcher_id):
    """æ›´æ–°ç ”ç©¶è€…çš„ç”¨æˆ·ä¿¡æ¯ï¼ˆå…³æ³¨è€…ç­‰æ•°æ®ï¼‰"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT name, x_account FROM researchers WHERE id = ?', (researcher_id,))
        researcher = cursor.fetchone()
        
        if not researcher:
            conn.close()
            return jsonify({'error': 'Researcher not found'}), 404
        
        name, x_account = researcher
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        user_info = twitter_api.get_user_info(x_account) if twitter_api else None
        
        if user_info:
            # æ›´æ–°æ•°æ®åº“ä¸­çš„ç”¨æˆ·ä¿¡æ¯ - ç›´æ¥å­˜å‚¨æ•°å­—è€Œä¸æ˜¯æ ¼å¼åŒ–å­—ç¬¦ä¸²
            cursor.execute('''
                UPDATE researchers 
                SET followers_count = ?, following_count = ?, updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            ''', (str(user_info['followers_count']), str(user_info['following_count']), researcher_id))
            
            conn.commit()
            conn.close()
            
            return jsonify({
                'message': f'æˆåŠŸæ›´æ–° {name} çš„ç”¨æˆ·ä¿¡æ¯',
                'user_info': user_info
            })
        else:
            conn.close()
            return jsonify({
                'message': f'æ— æ³•è·å– {name} çš„ç”¨æˆ·ä¿¡æ¯ï¼ˆå¯èƒ½æ˜¯APIé™åˆ¶æˆ–ç½‘ç»œé—®é¢˜ï¼‰',
                'user_info': None
            })
        
    except Exception as e:
        logger.error(f"æ›´æ–°ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/update_all_user_info', methods=['POST'])
def update_all_user_info():
    """æ‰¹é‡æ›´æ–°æ‰€æœ‰ç ”ç©¶è€…çš„ç”¨æˆ·ä¿¡æ¯"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT id, name, x_account FROM researchers ORDER BY id')
        researchers = cursor.fetchall()
        
        updated_count = 0
        failed_count = 0
        
        for researcher_id, name, x_account in researchers:
            try:
                # è·å–ç”¨æˆ·ä¿¡æ¯
                user_info = twitter_api.get_user_info(x_account) if twitter_api else None
                
                if user_info:
                    # æ›´æ–°æ•°æ®åº“
                    cursor.execute('''
                        UPDATE researchers 
                        SET followers_count = ?, following_count = ?, updated_at = CURRENT_TIMESTAMP
                        WHERE id = ?
                    ''', (str(user_info['followers_count']), str(user_info['following_count']), researcher_id))
                    
                    updated_count += 1
                    logger.info(f"âœ… æ›´æ–° {name}: {user_info['followers_count']} å…³æ³¨è€…, {user_info['following_count']} æ­£åœ¨å…³æ³¨")
                else:
                    failed_count += 1
                    logger.warning(f"âš ï¸ æ— æ³•è·å– {name} çš„ç”¨æˆ·ä¿¡æ¯")
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                time.sleep(2)  # å¢åŠ å»¶è¿Ÿåˆ°2ç§’
                
            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ æ›´æ–° {name} å¤±è´¥: {e}")
        
        conn.commit()
        conn.close()
        
        return jsonify({
            'message': f'æ‰¹é‡æ›´æ–°å®Œæˆ: æˆåŠŸ {updated_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª',
            'updated_count': updated_count,
            'failed_count': failed_count
        })
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ›´æ–°ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/fetch_historical/<int:researcher_id>', methods=['POST'])
def fetch_historical_content(researcher_id):
    """æŠ“å–ç ”ç©¶è€…çš„å†å²å†…å®¹"""
    try:
        data = request.get_json()
        start_date = data.get('start_date')  # æ ¼å¼: "2024-01-01"
        end_date = data.get('end_date', datetime.now().strftime('%Y-%m-%d'))
        max_results = data.get('max_results', 100)
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT name, x_account FROM researchers WHERE id = ?', (researcher_id,))
        researcher = cursor.fetchone()
        
        if not researcher:
            conn.close()
            return jsonify({'error': 'Researcher not found'}), 404
        
        name, x_account = researcher
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_time = datetime.strptime(start_date, '%Y-%m-%d').replace(tzinfo=timezone.utc) if start_date else None
        end_time = datetime.strptime(end_date, '%Y-%m-%d').replace(tzinfo=timezone.utc)
        
        logger.info(f"ğŸ” å¼€å§‹æŠ“å– {name} çš„å†å²å†…å®¹ï¼Œæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        
        # è·å–å†å²æ¨æ–‡
        tweets = twitter_api.get_user_tweets(
            x_account, 
            max_results=max_results,
            start_time=start_time,
            end_time=end_time
        ) if twitter_api else []
        
        new_content_count = 0
        if tweets:
            for tweet in tweets:
                # å­˜å‚¨åª’ä½“URLä¸ºJSONå­—ç¬¦ä¸²
                media_urls_json = json.dumps(tweet.get('media_urls', []))
                
                cursor.execute('''
                    INSERT OR IGNORE INTO x_content 
                    (researcher_id, tweet_id, content, likes_count, retweets_count, 
                     replies_count, created_at, is_historical, media_urls)
                    VALUES (?, ?, ?, ?, ?, ?, ?, 1, ?)
                ''', (
                    researcher_id, tweet['id'], tweet['content'],
                    tweet['likes'], tweet['retweets'], tweet['replies'],
                    tweet['created_at'], media_urls_json
                ))
                
                if cursor.rowcount > 0:
                    new_content_count += 1
            
            conn.commit()
        
        conn.close()
        
        period = f"{start_date} åˆ° {end_date}" if start_date else f"æ‰€æœ‰æ—¶é—´åˆ° {end_date}"
        message = f'æˆåŠŸæŠ“å– {name} åœ¨ {period} æœŸé—´çš„å†å²å†…å®¹'
        
        logger.info(f"âœ… {message}ï¼Œæ–°å¢ {new_content_count} æ¡ï¼Œæ€»è·å– {len(tweets)} æ¡")
        
        return jsonify({
            'message': message,
            'new_content_count': new_content_count,
            'total_fetched': len(tweets),
            'period': period
        })
        
    except Exception as e:
        logger.error(f"æŠ“å–å†å²å†…å®¹å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/historical_content/<int:researcher_id>')
def get_historical_content(researcher_id):
    """è·å–ç ”ç©¶è€…çš„å†å²å†…å®¹"""
    try:
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)
        per_page = min(per_page, 200)
        offset = (page - 1) * per_page
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # è·å–ç ”ç©¶è€…ä¿¡æ¯
        cursor.execute('SELECT * FROM researchers WHERE id = ?', (researcher_id,))
        researcher_row = cursor.fetchone()
        
        if not researcher_row:
            conn.close()
            return jsonify({'error': 'Researcher not found'}), 404
        
        researcher = {
            'id': researcher_row[0], 'name': researcher_row[2],
            'x_account': researcher_row[6], 'is_special_focus': bool(researcher_row[11])
        }
        
        # è·å–æ€»æ•°
        cursor.execute('SELECT COUNT(*) FROM x_content WHERE researcher_id = ?', (researcher_id,))
        total_count = cursor.fetchone()[0]
        
        # è·å–å†…å®¹
        cursor.execute('''
            SELECT id, content, likes_count, retweets_count, replies_count, 
                   created_at, collected_at, is_historical, media_urls
            FROM x_content 
            WHERE researcher_id = ? 
            ORDER BY created_at DESC 
            LIMIT ? OFFSET ?
        ''', (researcher_id, per_page, offset))
        
        content_list = []
        for row in cursor.fetchall():
            media_urls = []
            try:
                if row[8]:  # media_urls
                    media_urls = json.loads(row[8])
            except:
                pass
                
            content_list.append({
                'id': row[0],
                'content': row[1],
                'likes_count': row[2],
                'retweets_count': row[3],
                'replies_count': row[4],
                'created_at': row[5],
                'collected_at': row[6],
                'is_historical': bool(row[7]),
                'media_urls': media_urls
            })
        
        conn.close()
        
        return jsonify({
            'researcher': researcher,
            'content': content_list,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total_count,
                'pages': (total_count + per_page - 1) // per_page
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–å†å²å†…å®¹å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/export_word/<int:researcher_id>')
def export_to_word(researcher_id):
    """å¯¼å‡ºç ”ç©¶è€…å†…å®¹ä¸ºWordæ–‡æ¡£"""
    try:
        from docx import Document
        from docx.shared import Inches
        import io
        
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # è·å–ç ”ç©¶è€…ä¿¡æ¯
        cursor.execute('SELECT name, x_account, research_focus FROM researchers WHERE id = ?', (researcher_id,))
        researcher = cursor.fetchone()
        
        if not researcher:
            conn.close()
            return jsonify({'error': 'Researcher not found'}), 404
        
        name, x_account, research_focus = researcher
        
        # è·å–æ‰€æœ‰å†…å®¹
        cursor.execute('''
            SELECT content, likes_count, retweets_count, replies_count, 
                   created_at, media_urls
            FROM x_content 
            WHERE researcher_id = ? 
            ORDER BY created_at DESC
        ''', (researcher_id,))
        
        contents = cursor.fetchall()
        conn.close()
        
        # åˆ›å»ºWordæ–‡æ¡£
        doc = Document()
        
        # æ·»åŠ æ ‡é¢˜
        title = doc.add_heading(f'{name} å†…å®¹è®°å½•', 0)
        
        # æ·»åŠ åŸºæœ¬ä¿¡æ¯
        doc.add_heading('åŸºæœ¬ä¿¡æ¯', level=1)
        info_table = doc.add_table(rows=3, cols=2)
        info_table.style = 'Table Grid'
        
        info_table.cell(0, 0).text = 'å§“å'
        info_table.cell(0, 1).text = name
        info_table.cell(1, 0).text = 'Xè´¦å·'
        info_table.cell(1, 1).text = x_account
        info_table.cell(2, 0).text = 'ç ”ç©¶é¢†åŸŸ'
        info_table.cell(2, 1).text = research_focus or 'æœªçŸ¥'
        
        # æ·»åŠ å†…å®¹
        doc.add_heading('å†…å®¹è®°å½•', level=1)
        doc.add_paragraph(f'å…±æ”¶é›† {len(contents)} æ¡å†…å®¹ï¼ŒæŒ‰æ—¶é—´å€’åºæ’åˆ—ï¼š')
        
        for i, content in enumerate(contents, 1):
            text, likes, retweets, replies, created_at, media_urls = content
            
            # æ·»åŠ åºå·å’Œæ—¶é—´
            heading = doc.add_heading(f'{i}. {created_at[:19] if created_at else "æœªçŸ¥æ—¶é—´"}', level=2)
            
            # æ·»åŠ å†…å®¹
            doc.add_paragraph(text or 'æ— æ–‡å­—å†…å®¹')
            
            # æ·»åŠ åª’ä½“ä¿¡æ¯
            if media_urls:
                try:
                    media_list = json.loads(media_urls)
                    if media_list:
                        doc.add_paragraph('åŒ…å«åª’ä½“:')
                        for media in media_list:
                            media_type = media.get('type', 'unknown')
                            media_url = media.get('url', media.get('preview_url', ''))
                            if media_url:
                                doc.add_paragraph(f'â€¢ {media_type}: {media_url}', style='List Bullet')
                except:
                    pass
            
            # æ·»åŠ äº’åŠ¨æ•°æ®
            stats_p = doc.add_paragraph()
            stats_p.add_run(f'ğŸ‘ {likes} ç‚¹èµ  ğŸ”„ {retweets} è½¬å‘  ğŸ’¬ {replies} å›å¤')
            
            # æ·»åŠ åˆ†éš”çº¿
            if i < len(contents):
                doc.add_paragraph('â”€' * 50)
        
        # ä¿å­˜åˆ°å†…å­˜
        file_stream = io.BytesIO()
        doc.save(file_stream)
        file_stream.seek(0)
        
        filename = f"{name}_å†…å®¹è®°å½•_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
        
        return send_file(
            file_stream,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
        
    except ImportError:
        return jsonify({
            'error': 'Wordå¯¼å‡ºåŠŸèƒ½éœ€è¦å®‰è£… python-docx åº“',
            'solution': 'è¯·è¿è¡Œ: pip install python-docx'
        }), 500
    except Exception as e:
        logger.error(f"å¯¼å‡ºWordå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/database_status')
def get_database_status():
    """è·å–æ•°æ®åº“çŠ¶æ€ä¿¡æ¯"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        # æ£€æŸ¥å„è¡¨çš„è®°å½•æ•°
        cursor.execute('SELECT COUNT(*) FROM researchers')
        researchers_count = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM x_content')
        content_count = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM monitoring_tasks')
        tasks_count = cursor.fetchone()[0]
        
        # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶å¤§å°
        import os
        db_size = os.path.getsize('research_platform.db') if os.path.exists('research_platform.db') else 0
        
        # æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€
        cursor.execute('SELECT value FROM db_metadata WHERE key = ?', ('sample_data_loaded',))
        initialized = cursor.fetchone()
        
        conn.close()
        
        return jsonify({
            'database_file': 'research_platform.db',
            'file_exists': os.path.exists('research_platform.db'),
            'file_size_bytes': db_size,
            'file_size_mb': round(db_size / 1024 / 1024, 2),
            'tables': {
                'researchers': researchers_count,
                'x_content': content_count,
                'monitoring_tasks': tasks_count
            },
            'initialized': bool(initialized),
            'status': 'healthy' if researchers_count > 0 else 'empty'
        })
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'error': str(e), 'status': 'error'}), 500

@app.route('/api/system_status')
def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
    conn = sqlite3.connect('research_platform.db')
    cursor = conn.cursor()
    
    # æ•°æ®åº“ç»Ÿè®¡
    cursor.execute('SELECT COUNT(*) FROM researchers')
    total_researchers = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM researchers WHERE is_monitoring = 1')
    monitoring_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM x_content')
    total_content = cursor.fetchone()[0]
    
    # æœ€è¿‘24å°æ—¶çš„æ´»åŠ¨
    cursor.execute('''
        SELECT COUNT(*) FROM x_content 
        WHERE collected_at >= datetime('now', '-1 day')
    ''')
    recent_content = cursor.fetchone()[0]
    
    # è·å–ç›‘æ§é—´éš”
    monitoring_interval = 1800
    try:
        cursor.execute('SELECT setting_value FROM system_settings WHERE setting_key = ?', ('monitoring_interval',))
        result = cursor.fetchone()
        if result:
            monitoring_interval = int(result[0])
    except Exception as e:
        logger.error(f"è·å–ç›‘æ§é—´éš”å¤±è´¥: {e}")
    
    conn.close()
    
    return jsonify({
        'system_capacity': {
            'max_researchers': 5000,
            'current_researchers': total_researchers,
            'available_slots': 5000 - total_researchers,
            'utilization_percentage': (total_researchers / 5000) * 100
        },
        'monitoring_status': {
            'active_monitoring': monitoring_count,
            'max_concurrent': 1000,
            'service_running': monitoring_service and monitoring_service.running,
            'monitoring_interval': monitoring_interval,
            'interval_display': format_interval(monitoring_interval)
        },
        'data_statistics': {
            'total_content': total_content,
            'recent_24h': recent_content
        },
        'api_status': {
            'twitter_connected': twitter_api and twitter_api.client is not None,
            'twitter_working': twitter_api and twitter_api.api_working,
            'last_check': datetime.now().isoformat()
        }
    })

@app.route('/api/reset_sample_data', methods=['POST'])
def reset_sample_data():
    """é‡ç½®ç¤ºä¾‹æ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•å’Œæ¢å¤ï¼‰"""
    try:
        if researcher_manager:
            conn = sqlite3.connect('research_platform.db')
            cursor = conn.cursor()
            
            # åˆ é™¤ç°æœ‰ç¤ºä¾‹æ•°æ®ï¼ˆåŸºäºåå­—åˆ¤æ–­ï¼‰
            sample_names = ['Ilya Sutskever', 'Noam Shazeer', 'Geoffrey Hinton', 'Alec Radford', 'Andrej Karpathy']
            for name in sample_names:
                cursor.execute('DELETE FROM researchers WHERE name = ?', (name,))
            
            # é‡ç½®åˆå§‹åŒ–æ ‡è®°
            cursor.execute('DELETE FROM db_metadata WHERE key = ?', ('sample_data_loaded',))
            
            conn.commit()
            conn.close()
            
            # é‡æ–°åŠ è½½ç¤ºä¾‹æ•°æ®
            researcher_manager.load_sample_data_if_empty()
            
            return jsonify({'message': 'ç¤ºä¾‹æ•°æ®å·²é‡ç½®', 'status': 'success'})
        else:
            return jsonify({'error': 'ç ”ç©¶è€…ç®¡ç†å™¨æœªåˆå§‹åŒ–'}), 500
            
    except Exception as e:
        logger.error(f"é‡ç½®ç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy' if researcher_manager else 'partial',
        'timestamp': datetime.now().isoformat(),
        'twitter_api': 'connected' if twitter_api and twitter_api.client else 'disconnected',
        'twitter_working': twitter_api and twitter_api.api_working,
        'monitoring': 'active' if monitoring_service and monitoring_service.running else 'inactive',
        'capacity': '5000 researchers supported',
        'components': {
            'researcher_manager': 'ok' if researcher_manager else 'failed',
            'twitter_api': 'ok' if twitter_api else 'failed',
            'monitoring_service': 'ok' if monitoring_service else 'failed'
        }
    })

@app.route('/api/init_status')
def get_init_status():
    """è·å–åˆå§‹åŒ–çŠ¶æ€"""
    return jsonify({
        'initialized': bool(researcher_manager),
        'components': {
            'database': bool(researcher_manager),
            'twitter_api': bool(twitter_api),
            'monitoring': bool(monitoring_service)
        },
        'ready': bool(researcher_manager and twitter_api and monitoring_service)
    })

@app.route('/api/test_twitter/<int:researcher_id>', methods=['POST'])
def test_twitter_api(researcher_id):
    """æµ‹è¯•Twitter APIè¿æ¥å’Œæ•°æ®è·å–"""
    try:
        conn = sqlite3.connect('research_platform.db')
        cursor = conn.cursor()
        
        cursor.execute('SELECT name, x_account FROM researchers WHERE id = ?', (researcher_id,))
        researcher = cursor.fetchone()
        
        if not researcher:
            conn.close()
            return jsonify({'error': 'Researcher not found'}), 404
        
        name, x_account = researcher
        conn.close()
        
        # æµ‹è¯•APIè¿æ¥
        if not twitter_api:
            return jsonify({
                'error': 'Twitter APIæœªåˆå§‹åŒ–',
                'name': name,
                'x_account': x_account
            }), 500
        
        # æµ‹è¯•è·å–ç”¨æˆ·ä¿¡æ¯
        logger.info(f"ğŸ§ª æµ‹è¯•è·å– {name} ({x_account}) çš„ç”¨æˆ·ä¿¡æ¯")
        user_info = twitter_api.get_user_info(x_account)
        
        # æµ‹è¯•è·å–æ¨æ–‡
        logger.info(f"ğŸ§ª æµ‹è¯•è·å– {name} ({x_account}) çš„æ¨æ–‡")
        tweets = twitter_api.get_user_tweets(x_account, max_results=5)
        
        return jsonify({
            'message': f'æµ‹è¯•å®Œæˆ',
            'name': name,
            'x_account': x_account,
            'api_working': twitter_api.api_working,
            'user_info': user_info,
            'tweets_count': len(tweets) if tweets else 0,
            'tweets_sample': tweets[:2] if tweets else [],  # è¿”å›å‰2æ¡ä½œä¸ºæ ·æœ¬
            'test_results': {
                'user_info_success': user_info is not None,
                'tweets_success': tweets is not None and len(tweets) > 0
            }
        })
        
    except Exception as e:
        logger.error(f"æµ‹è¯•Twitter APIå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ AIç ”ç©¶è€…Xå†…å®¹å­¦ä¹ å¹³å°å¯åŠ¨ä¸­...")
    logger.info(f"ğŸ“Š ç³»ç»Ÿå®¹é‡: æœ€å¤§æ”¯æŒ 5000 ä½ç ”ç©¶è€…ç›‘æ§")
    logger.info(f"ğŸ’¾ æ•°æ®åº“æ–‡ä»¶: research_platform.db")
    
    # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
    if researcher_manager:
        import os
        if os.path.exists('research_platform.db'):
            conn = sqlite3.connect('research_platform.db')
            cursor = conn.cursor()
            cursor.execute('SELECT COUNT(*) FROM researchers')
            count = cursor.fetchone()[0]
            conn.close()
            logger.info(f"ğŸ“‹ æ•°æ®åº“çŠ¶æ€: å·²æœ‰ {count} ä½ç ”ç©¶è€…")
        else:
            logger.info("ğŸ“‹ æ•°æ®åº“çŠ¶æ€: æ–°å»ºæ•°æ®åº“")
    
    logger.info(f"ğŸ”‘ Twitter API: {'âœ… å·²é…ç½®' if TWITTER_BEARER_TOKEN else 'âš ï¸ æœªé…ç½®ï¼Œå°†æ— æ³•è·å–çœŸå®æ•°æ®'}")
    
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
